// Code generated by pulumi-language-go DO NOT EDIT.
// *** WARNING: Do not edit by hand unless you're certain you know what you are doing! ***

package mongodbatlas

import (
	"context"
	"reflect"

	"errors"
	"github.com/pulumi/pulumi-mongodbatlas/sdk/v3/go/mongodbatlas/internal"
	"github.com/pulumi/pulumi/sdk/v3/go/pulumi"
)

// **WARNING:** Data Lake is deprecated. To learn more, see <https://dochub.mongodb.org/core/data-lake-deprecation>
//
// `DataLakePipeline` provides a Data Lake Pipeline resource.
//
// > **NOTE:** Groups and projects are synonymous terms. You may find `groupId` in the official documentation.
//
// ## Example Usage
//
// ### S
//
// ## Import
//
// Data Lake Pipeline can be imported using project ID, name of the data lake and name of the AWS s3 bucket, in the format `project_id`--`name`, e.g.
//
// ```sh
// $ pulumi import mongodbatlas:index/dataLakePipeline:DataLakePipeline example 1112222b3bf99403840e8934--test-data-lake-pipeline-test
// ```
//
// See [MongoDB Atlas API](https://www.mongodb.com/docs/atlas/reference/api-resources-spec/#tag/Data-Lake-Pipelines) Documentation for more information.
type DataLakePipeline struct {
	pulumi.CustomResourceState

	// Timestamp that indicates when the Data Lake Pipeline was created.
	CreatedDate pulumi.StringOutput `pulumi:"createdDate"`
	// List of backup schedule policy items that you can use as a Data Lake Pipeline source.
	// * `ingestion_schedules.#.id` - Unique 24-hexadecimal digit string that identifies this backup policy item.
	// * `ingestion_schedules.#.frequency_type` - Human-readable label that identifies the frequency type associated with the backup policy.
	// * `ingestion_schedules.#.frequency_interval` - Number that indicates the frequency interval for a set of snapshots.
	// * `ingestion_schedules.#.retention_unit` - Unit of time in which MongoDB Atlas measures snapshot retention.
	// * `ingestion_schedules.#.retention_value` - Duration in days, weeks, or months that MongoDB Atlas retains the snapshot.
	IngestionSchedules DataLakePipelineIngestionScheduleArrayOutput `pulumi:"ingestionSchedules"`
	// Timestamp that indicates the last time that the Data Lake Pipeline was updated.
	LastUpdatedDate pulumi.StringOutput `pulumi:"lastUpdatedDate"`
	// Name of the Atlas Data Lake Pipeline.
	Name pulumi.StringOutput `pulumi:"name"`
	// The unique ID for the project to create a data lake pipeline.
	ProjectId pulumi.StringOutput           `pulumi:"projectId"`
	Sink      DataLakePipelineSinkPtrOutput `pulumi:"sink"`
	// List of backup snapshots that you can use to trigger an on demand pipeline run.
	// * `snapshots.#.id` - Unique 24-hexadecimal digit string that identifies the snapshot.
	// * `snapshots.#.provider` - Human-readable label that identifies the cloud provider that stores this snapshot.
	// * `snapshots.#.created_at` - Date and time when MongoDB Atlas took the snapshot.
	// * `snapshots.#.expires_at` - Date and time when MongoDB Atlas deletes the snapshot.
	// * `snapshots.#.frequency_type` - Human-readable label that identifies how often this snapshot triggers.
	// * `snapshots.#.master_key` - Unique string that identifies the Amazon Web Services (AWS) Key Management Service (KMS) Customer Master Key (CMK) used to encrypt the snapshot.
	// * `snapshots.#.mongod_version` - Version of the MongoDB host that this snapshot backs up.
	// * `snapshots.#.replica_set_name` - Human-readable label that identifies the replica set from which MongoDB Atlas took this snapshot.
	// * `snapshots.#.type` - Human-readable label that categorizes the cluster as a replica set or sharded cluster.
	// * `snapshots.#.snapshot_type` - Human-readable label that identifies when this snapshot triggers.
	// * `snapshots.#.status` - Human-readable label that indicates the stage of the backup process for this snapshot.
	// * `snapshots.#.size` - List of backup snapshots that you can use to trigger an on demand pipeline run.
	// * `snapshots.#.copy_region` - List that identifies the regions to which MongoDB Atlas copies the snapshot.
	// * `snapshots.#.policies` - List that contains unique identifiers for the policy items.
	Snapshots DataLakePipelineSnapshotArrayOutput `pulumi:"snapshots"`
	Source    DataLakePipelineSourcePtrOutput     `pulumi:"source"`
	// State of this Data Lake Pipeline.
	State pulumi.StringOutput `pulumi:"state"`
	// Fields to be excluded for this Data Lake Pipeline.
	// * `transformations.#.field` - Key in the document.
	// * `transformations.#.type` - Type of transformation applied during the export of the namespace in a Data Lake Pipeline.
	Transformations DataLakePipelineTransformationArrayOutput `pulumi:"transformations"`
}

// NewDataLakePipeline registers a new resource with the given unique name, arguments, and options.
func NewDataLakePipeline(ctx *pulumi.Context,
	name string, args *DataLakePipelineArgs, opts ...pulumi.ResourceOption) (*DataLakePipeline, error) {
	if args == nil {
		return nil, errors.New("missing one or more required arguments")
	}

	if args.ProjectId == nil {
		return nil, errors.New("invalid value for required argument 'ProjectId'")
	}
	opts = internal.PkgResourceDefaultOpts(opts)
	var resource DataLakePipeline
	err := ctx.RegisterResource("mongodbatlas:index/dataLakePipeline:DataLakePipeline", name, args, &resource, opts...)
	if err != nil {
		return nil, err
	}
	return &resource, nil
}

// GetDataLakePipeline gets an existing DataLakePipeline resource's state with the given name, ID, and optional
// state properties that are used to uniquely qualify the lookup (nil if not required).
func GetDataLakePipeline(ctx *pulumi.Context,
	name string, id pulumi.IDInput, state *DataLakePipelineState, opts ...pulumi.ResourceOption) (*DataLakePipeline, error) {
	var resource DataLakePipeline
	err := ctx.ReadResource("mongodbatlas:index/dataLakePipeline:DataLakePipeline", name, id, state, &resource, opts...)
	if err != nil {
		return nil, err
	}
	return &resource, nil
}

// Input properties used for looking up and filtering DataLakePipeline resources.
type dataLakePipelineState struct {
	// Timestamp that indicates when the Data Lake Pipeline was created.
	CreatedDate *string `pulumi:"createdDate"`
	// List of backup schedule policy items that you can use as a Data Lake Pipeline source.
	// * `ingestion_schedules.#.id` - Unique 24-hexadecimal digit string that identifies this backup policy item.
	// * `ingestion_schedules.#.frequency_type` - Human-readable label that identifies the frequency type associated with the backup policy.
	// * `ingestion_schedules.#.frequency_interval` - Number that indicates the frequency interval for a set of snapshots.
	// * `ingestion_schedules.#.retention_unit` - Unit of time in which MongoDB Atlas measures snapshot retention.
	// * `ingestion_schedules.#.retention_value` - Duration in days, weeks, or months that MongoDB Atlas retains the snapshot.
	IngestionSchedules []DataLakePipelineIngestionSchedule `pulumi:"ingestionSchedules"`
	// Timestamp that indicates the last time that the Data Lake Pipeline was updated.
	LastUpdatedDate *string `pulumi:"lastUpdatedDate"`
	// Name of the Atlas Data Lake Pipeline.
	Name *string `pulumi:"name"`
	// The unique ID for the project to create a data lake pipeline.
	ProjectId *string               `pulumi:"projectId"`
	Sink      *DataLakePipelineSink `pulumi:"sink"`
	// List of backup snapshots that you can use to trigger an on demand pipeline run.
	// * `snapshots.#.id` - Unique 24-hexadecimal digit string that identifies the snapshot.
	// * `snapshots.#.provider` - Human-readable label that identifies the cloud provider that stores this snapshot.
	// * `snapshots.#.created_at` - Date and time when MongoDB Atlas took the snapshot.
	// * `snapshots.#.expires_at` - Date and time when MongoDB Atlas deletes the snapshot.
	// * `snapshots.#.frequency_type` - Human-readable label that identifies how often this snapshot triggers.
	// * `snapshots.#.master_key` - Unique string that identifies the Amazon Web Services (AWS) Key Management Service (KMS) Customer Master Key (CMK) used to encrypt the snapshot.
	// * `snapshots.#.mongod_version` - Version of the MongoDB host that this snapshot backs up.
	// * `snapshots.#.replica_set_name` - Human-readable label that identifies the replica set from which MongoDB Atlas took this snapshot.
	// * `snapshots.#.type` - Human-readable label that categorizes the cluster as a replica set or sharded cluster.
	// * `snapshots.#.snapshot_type` - Human-readable label that identifies when this snapshot triggers.
	// * `snapshots.#.status` - Human-readable label that indicates the stage of the backup process for this snapshot.
	// * `snapshots.#.size` - List of backup snapshots that you can use to trigger an on demand pipeline run.
	// * `snapshots.#.copy_region` - List that identifies the regions to which MongoDB Atlas copies the snapshot.
	// * `snapshots.#.policies` - List that contains unique identifiers for the policy items.
	Snapshots []DataLakePipelineSnapshot `pulumi:"snapshots"`
	Source    *DataLakePipelineSource    `pulumi:"source"`
	// State of this Data Lake Pipeline.
	State *string `pulumi:"state"`
	// Fields to be excluded for this Data Lake Pipeline.
	// * `transformations.#.field` - Key in the document.
	// * `transformations.#.type` - Type of transformation applied during the export of the namespace in a Data Lake Pipeline.
	Transformations []DataLakePipelineTransformation `pulumi:"transformations"`
}

type DataLakePipelineState struct {
	// Timestamp that indicates when the Data Lake Pipeline was created.
	CreatedDate pulumi.StringPtrInput
	// List of backup schedule policy items that you can use as a Data Lake Pipeline source.
	// * `ingestion_schedules.#.id` - Unique 24-hexadecimal digit string that identifies this backup policy item.
	// * `ingestion_schedules.#.frequency_type` - Human-readable label that identifies the frequency type associated with the backup policy.
	// * `ingestion_schedules.#.frequency_interval` - Number that indicates the frequency interval for a set of snapshots.
	// * `ingestion_schedules.#.retention_unit` - Unit of time in which MongoDB Atlas measures snapshot retention.
	// * `ingestion_schedules.#.retention_value` - Duration in days, weeks, or months that MongoDB Atlas retains the snapshot.
	IngestionSchedules DataLakePipelineIngestionScheduleArrayInput
	// Timestamp that indicates the last time that the Data Lake Pipeline was updated.
	LastUpdatedDate pulumi.StringPtrInput
	// Name of the Atlas Data Lake Pipeline.
	Name pulumi.StringPtrInput
	// The unique ID for the project to create a data lake pipeline.
	ProjectId pulumi.StringPtrInput
	Sink      DataLakePipelineSinkPtrInput
	// List of backup snapshots that you can use to trigger an on demand pipeline run.
	// * `snapshots.#.id` - Unique 24-hexadecimal digit string that identifies the snapshot.
	// * `snapshots.#.provider` - Human-readable label that identifies the cloud provider that stores this snapshot.
	// * `snapshots.#.created_at` - Date and time when MongoDB Atlas took the snapshot.
	// * `snapshots.#.expires_at` - Date and time when MongoDB Atlas deletes the snapshot.
	// * `snapshots.#.frequency_type` - Human-readable label that identifies how often this snapshot triggers.
	// * `snapshots.#.master_key` - Unique string that identifies the Amazon Web Services (AWS) Key Management Service (KMS) Customer Master Key (CMK) used to encrypt the snapshot.
	// * `snapshots.#.mongod_version` - Version of the MongoDB host that this snapshot backs up.
	// * `snapshots.#.replica_set_name` - Human-readable label that identifies the replica set from which MongoDB Atlas took this snapshot.
	// * `snapshots.#.type` - Human-readable label that categorizes the cluster as a replica set or sharded cluster.
	// * `snapshots.#.snapshot_type` - Human-readable label that identifies when this snapshot triggers.
	// * `snapshots.#.status` - Human-readable label that indicates the stage of the backup process for this snapshot.
	// * `snapshots.#.size` - List of backup snapshots that you can use to trigger an on demand pipeline run.
	// * `snapshots.#.copy_region` - List that identifies the regions to which MongoDB Atlas copies the snapshot.
	// * `snapshots.#.policies` - List that contains unique identifiers for the policy items.
	Snapshots DataLakePipelineSnapshotArrayInput
	Source    DataLakePipelineSourcePtrInput
	// State of this Data Lake Pipeline.
	State pulumi.StringPtrInput
	// Fields to be excluded for this Data Lake Pipeline.
	// * `transformations.#.field` - Key in the document.
	// * `transformations.#.type` - Type of transformation applied during the export of the namespace in a Data Lake Pipeline.
	Transformations DataLakePipelineTransformationArrayInput
}

func (DataLakePipelineState) ElementType() reflect.Type {
	return reflect.TypeOf((*dataLakePipelineState)(nil)).Elem()
}

type dataLakePipelineArgs struct {
	// Name of the Atlas Data Lake Pipeline.
	Name *string `pulumi:"name"`
	// The unique ID for the project to create a data lake pipeline.
	ProjectId string                  `pulumi:"projectId"`
	Sink      *DataLakePipelineSink   `pulumi:"sink"`
	Source    *DataLakePipelineSource `pulumi:"source"`
	// Fields to be excluded for this Data Lake Pipeline.
	// * `transformations.#.field` - Key in the document.
	// * `transformations.#.type` - Type of transformation applied during the export of the namespace in a Data Lake Pipeline.
	Transformations []DataLakePipelineTransformation `pulumi:"transformations"`
}

// The set of arguments for constructing a DataLakePipeline resource.
type DataLakePipelineArgs struct {
	// Name of the Atlas Data Lake Pipeline.
	Name pulumi.StringPtrInput
	// The unique ID for the project to create a data lake pipeline.
	ProjectId pulumi.StringInput
	Sink      DataLakePipelineSinkPtrInput
	Source    DataLakePipelineSourcePtrInput
	// Fields to be excluded for this Data Lake Pipeline.
	// * `transformations.#.field` - Key in the document.
	// * `transformations.#.type` - Type of transformation applied during the export of the namespace in a Data Lake Pipeline.
	Transformations DataLakePipelineTransformationArrayInput
}

func (DataLakePipelineArgs) ElementType() reflect.Type {
	return reflect.TypeOf((*dataLakePipelineArgs)(nil)).Elem()
}

type DataLakePipelineInput interface {
	pulumi.Input

	ToDataLakePipelineOutput() DataLakePipelineOutput
	ToDataLakePipelineOutputWithContext(ctx context.Context) DataLakePipelineOutput
}

func (*DataLakePipeline) ElementType() reflect.Type {
	return reflect.TypeOf((**DataLakePipeline)(nil)).Elem()
}

func (i *DataLakePipeline) ToDataLakePipelineOutput() DataLakePipelineOutput {
	return i.ToDataLakePipelineOutputWithContext(context.Background())
}

func (i *DataLakePipeline) ToDataLakePipelineOutputWithContext(ctx context.Context) DataLakePipelineOutput {
	return pulumi.ToOutputWithContext(ctx, i).(DataLakePipelineOutput)
}

// DataLakePipelineArrayInput is an input type that accepts DataLakePipelineArray and DataLakePipelineArrayOutput values.
// You can construct a concrete instance of `DataLakePipelineArrayInput` via:
//
//	DataLakePipelineArray{ DataLakePipelineArgs{...} }
type DataLakePipelineArrayInput interface {
	pulumi.Input

	ToDataLakePipelineArrayOutput() DataLakePipelineArrayOutput
	ToDataLakePipelineArrayOutputWithContext(context.Context) DataLakePipelineArrayOutput
}

type DataLakePipelineArray []DataLakePipelineInput

func (DataLakePipelineArray) ElementType() reflect.Type {
	return reflect.TypeOf((*[]*DataLakePipeline)(nil)).Elem()
}

func (i DataLakePipelineArray) ToDataLakePipelineArrayOutput() DataLakePipelineArrayOutput {
	return i.ToDataLakePipelineArrayOutputWithContext(context.Background())
}

func (i DataLakePipelineArray) ToDataLakePipelineArrayOutputWithContext(ctx context.Context) DataLakePipelineArrayOutput {
	return pulumi.ToOutputWithContext(ctx, i).(DataLakePipelineArrayOutput)
}

// DataLakePipelineMapInput is an input type that accepts DataLakePipelineMap and DataLakePipelineMapOutput values.
// You can construct a concrete instance of `DataLakePipelineMapInput` via:
//
//	DataLakePipelineMap{ "key": DataLakePipelineArgs{...} }
type DataLakePipelineMapInput interface {
	pulumi.Input

	ToDataLakePipelineMapOutput() DataLakePipelineMapOutput
	ToDataLakePipelineMapOutputWithContext(context.Context) DataLakePipelineMapOutput
}

type DataLakePipelineMap map[string]DataLakePipelineInput

func (DataLakePipelineMap) ElementType() reflect.Type {
	return reflect.TypeOf((*map[string]*DataLakePipeline)(nil)).Elem()
}

func (i DataLakePipelineMap) ToDataLakePipelineMapOutput() DataLakePipelineMapOutput {
	return i.ToDataLakePipelineMapOutputWithContext(context.Background())
}

func (i DataLakePipelineMap) ToDataLakePipelineMapOutputWithContext(ctx context.Context) DataLakePipelineMapOutput {
	return pulumi.ToOutputWithContext(ctx, i).(DataLakePipelineMapOutput)
}

type DataLakePipelineOutput struct{ *pulumi.OutputState }

func (DataLakePipelineOutput) ElementType() reflect.Type {
	return reflect.TypeOf((**DataLakePipeline)(nil)).Elem()
}

func (o DataLakePipelineOutput) ToDataLakePipelineOutput() DataLakePipelineOutput {
	return o
}

func (o DataLakePipelineOutput) ToDataLakePipelineOutputWithContext(ctx context.Context) DataLakePipelineOutput {
	return o
}

// Timestamp that indicates when the Data Lake Pipeline was created.
func (o DataLakePipelineOutput) CreatedDate() pulumi.StringOutput {
	return o.ApplyT(func(v *DataLakePipeline) pulumi.StringOutput { return v.CreatedDate }).(pulumi.StringOutput)
}

// List of backup schedule policy items that you can use as a Data Lake Pipeline source.
// * `ingestion_schedules.#.id` - Unique 24-hexadecimal digit string that identifies this backup policy item.
// * `ingestion_schedules.#.frequency_type` - Human-readable label that identifies the frequency type associated with the backup policy.
// * `ingestion_schedules.#.frequency_interval` - Number that indicates the frequency interval for a set of snapshots.
// * `ingestion_schedules.#.retention_unit` - Unit of time in which MongoDB Atlas measures snapshot retention.
// * `ingestion_schedules.#.retention_value` - Duration in days, weeks, or months that MongoDB Atlas retains the snapshot.
func (o DataLakePipelineOutput) IngestionSchedules() DataLakePipelineIngestionScheduleArrayOutput {
	return o.ApplyT(func(v *DataLakePipeline) DataLakePipelineIngestionScheduleArrayOutput { return v.IngestionSchedules }).(DataLakePipelineIngestionScheduleArrayOutput)
}

// Timestamp that indicates the last time that the Data Lake Pipeline was updated.
func (o DataLakePipelineOutput) LastUpdatedDate() pulumi.StringOutput {
	return o.ApplyT(func(v *DataLakePipeline) pulumi.StringOutput { return v.LastUpdatedDate }).(pulumi.StringOutput)
}

// Name of the Atlas Data Lake Pipeline.
func (o DataLakePipelineOutput) Name() pulumi.StringOutput {
	return o.ApplyT(func(v *DataLakePipeline) pulumi.StringOutput { return v.Name }).(pulumi.StringOutput)
}

// The unique ID for the project to create a data lake pipeline.
func (o DataLakePipelineOutput) ProjectId() pulumi.StringOutput {
	return o.ApplyT(func(v *DataLakePipeline) pulumi.StringOutput { return v.ProjectId }).(pulumi.StringOutput)
}

func (o DataLakePipelineOutput) Sink() DataLakePipelineSinkPtrOutput {
	return o.ApplyT(func(v *DataLakePipeline) DataLakePipelineSinkPtrOutput { return v.Sink }).(DataLakePipelineSinkPtrOutput)
}

// List of backup snapshots that you can use to trigger an on demand pipeline run.
// * `snapshots.#.id` - Unique 24-hexadecimal digit string that identifies the snapshot.
// * `snapshots.#.provider` - Human-readable label that identifies the cloud provider that stores this snapshot.
// * `snapshots.#.created_at` - Date and time when MongoDB Atlas took the snapshot.
// * `snapshots.#.expires_at` - Date and time when MongoDB Atlas deletes the snapshot.
// * `snapshots.#.frequency_type` - Human-readable label that identifies how often this snapshot triggers.
// * `snapshots.#.master_key` - Unique string that identifies the Amazon Web Services (AWS) Key Management Service (KMS) Customer Master Key (CMK) used to encrypt the snapshot.
// * `snapshots.#.mongod_version` - Version of the MongoDB host that this snapshot backs up.
// * `snapshots.#.replica_set_name` - Human-readable label that identifies the replica set from which MongoDB Atlas took this snapshot.
// * `snapshots.#.type` - Human-readable label that categorizes the cluster as a replica set or sharded cluster.
// * `snapshots.#.snapshot_type` - Human-readable label that identifies when this snapshot triggers.
// * `snapshots.#.status` - Human-readable label that indicates the stage of the backup process for this snapshot.
// * `snapshots.#.size` - List of backup snapshots that you can use to trigger an on demand pipeline run.
// * `snapshots.#.copy_region` - List that identifies the regions to which MongoDB Atlas copies the snapshot.
// * `snapshots.#.policies` - List that contains unique identifiers for the policy items.
func (o DataLakePipelineOutput) Snapshots() DataLakePipelineSnapshotArrayOutput {
	return o.ApplyT(func(v *DataLakePipeline) DataLakePipelineSnapshotArrayOutput { return v.Snapshots }).(DataLakePipelineSnapshotArrayOutput)
}

func (o DataLakePipelineOutput) Source() DataLakePipelineSourcePtrOutput {
	return o.ApplyT(func(v *DataLakePipeline) DataLakePipelineSourcePtrOutput { return v.Source }).(DataLakePipelineSourcePtrOutput)
}

// State of this Data Lake Pipeline.
func (o DataLakePipelineOutput) State() pulumi.StringOutput {
	return o.ApplyT(func(v *DataLakePipeline) pulumi.StringOutput { return v.State }).(pulumi.StringOutput)
}

// Fields to be excluded for this Data Lake Pipeline.
// * `transformations.#.field` - Key in the document.
// * `transformations.#.type` - Type of transformation applied during the export of the namespace in a Data Lake Pipeline.
func (o DataLakePipelineOutput) Transformations() DataLakePipelineTransformationArrayOutput {
	return o.ApplyT(func(v *DataLakePipeline) DataLakePipelineTransformationArrayOutput { return v.Transformations }).(DataLakePipelineTransformationArrayOutput)
}

type DataLakePipelineArrayOutput struct{ *pulumi.OutputState }

func (DataLakePipelineArrayOutput) ElementType() reflect.Type {
	return reflect.TypeOf((*[]*DataLakePipeline)(nil)).Elem()
}

func (o DataLakePipelineArrayOutput) ToDataLakePipelineArrayOutput() DataLakePipelineArrayOutput {
	return o
}

func (o DataLakePipelineArrayOutput) ToDataLakePipelineArrayOutputWithContext(ctx context.Context) DataLakePipelineArrayOutput {
	return o
}

func (o DataLakePipelineArrayOutput) Index(i pulumi.IntInput) DataLakePipelineOutput {
	return pulumi.All(o, i).ApplyT(func(vs []interface{}) *DataLakePipeline {
		return vs[0].([]*DataLakePipeline)[vs[1].(int)]
	}).(DataLakePipelineOutput)
}

type DataLakePipelineMapOutput struct{ *pulumi.OutputState }

func (DataLakePipelineMapOutput) ElementType() reflect.Type {
	return reflect.TypeOf((*map[string]*DataLakePipeline)(nil)).Elem()
}

func (o DataLakePipelineMapOutput) ToDataLakePipelineMapOutput() DataLakePipelineMapOutput {
	return o
}

func (o DataLakePipelineMapOutput) ToDataLakePipelineMapOutputWithContext(ctx context.Context) DataLakePipelineMapOutput {
	return o
}

func (o DataLakePipelineMapOutput) MapIndex(k pulumi.StringInput) DataLakePipelineOutput {
	return pulumi.All(o, k).ApplyT(func(vs []interface{}) *DataLakePipeline {
		return vs[0].(map[string]*DataLakePipeline)[vs[1].(string)]
	}).(DataLakePipelineOutput)
}

func init() {
	pulumi.RegisterInputType(reflect.TypeOf((*DataLakePipelineInput)(nil)).Elem(), &DataLakePipeline{})
	pulumi.RegisterInputType(reflect.TypeOf((*DataLakePipelineArrayInput)(nil)).Elem(), DataLakePipelineArray{})
	pulumi.RegisterInputType(reflect.TypeOf((*DataLakePipelineMapInput)(nil)).Elem(), DataLakePipelineMap{})
	pulumi.RegisterOutputType(DataLakePipelineOutput{})
	pulumi.RegisterOutputType(DataLakePipelineArrayOutput{})
	pulumi.RegisterOutputType(DataLakePipelineMapOutput{})
}

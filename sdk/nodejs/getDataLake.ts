// *** WARNING: this file was generated by the Pulumi Terraform Bridge (tfgen) Tool. ***
// *** Do not edit by hand unless you're certain you know what you are doing! ***

import * as pulumi from "@pulumi/pulumi";
import { input as inputs, output as outputs } from "./types";
import * as utilities from "./utilities";

/**
 * `mongodbatlas.DataLake` describe a Data Lake.
 *
 * > **NOTE:** Groups and projects are synonymous terms. You may find groupId in the official documentation.
 */
export function getDataLake(args: GetDataLakeArgs, opts?: pulumi.InvokeOptions): Promise<GetDataLakeResult> {
    if (!opts) {
        opts = {}
    }

    opts = pulumi.mergeOptions(utilities.resourceOptsDefaults(), opts);
    return pulumi.runtime.invoke("mongodbatlas:index/getDataLake:getDataLake", {
        "name": args.name,
        "projectId": args.projectId,
    }, opts);
}

/**
 * A collection of arguments for invoking getDataLake.
 */
export interface GetDataLakeArgs {
    /**
     * Name of the data lake.
     */
    name: string;
    /**
     * The unique ID for the project to create a data lake.
     */
    projectId: string;
}

/**
 * A collection of values returned by getDataLake.
 */
export interface GetDataLakeResult {
    /**
     * AWS provider of the cloud service where Data Lake can access the S3 Bucket.
     * * `aws.0.role_id` - Unique identifier of the role that Data Lake can use to access the data stores.
     * * `aws.0.test_s3_bucket` - Name of the S3 data bucket that the provided role ID is authorized to access.
     * * `aws.0.role_id` - Unique identifier of the role that Data Lake can use to access the data stores.
     * * `aws.0.test_s3_bucket` - Name of the S3 data bucket that the provided role ID is authorized to access.
     * * `aws.0.iam_assumed_role_arn` - Amazon Resource Name (ARN) of the IAM Role that Data Lake assumes when accessing S3 Bucket data stores.
     */
    readonly aws: outputs.GetDataLakeAw[];
    /**
     * The cloud provider region to which Atlas Data Lake routes client connections for data processing.
     * * `data_process_region.0.cloud_provider` - Name of the cloud service provider.
     * * `data_process_region.0.region` -Name of the region to which Data Lake routes client connections for data processing.
     */
    readonly dataProcessRegions: outputs.GetDataLakeDataProcessRegion[];
    /**
     * The list of hostnames assigned to the Atlas Data Lake. Each string in the array is a hostname assigned to the Atlas Data Lake.
     */
    readonly hostnames: string[];
    /**
     * The provider-assigned unique ID for this managed resource.
     */
    readonly id: string;
    readonly name: string;
    readonly projectId: string;
    /**
     * Current state of the Atlas Data Lake:
     */
    readonly state: string;
    /**
     * Configuration details for mapping each data store to queryable databases and collections.
     * * `storage_databases.#.name` - Name of the database to which Data Lake maps the data contained in the data store.
     * * `storage_databases.#.collections` -     Array of objects where each object represents a collection and data sources that map to a [stores](https://docs.mongodb.com/datalake/reference/format/data-lake-configuration#mongodb-datalakeconf-datalakeconf.stores) data store.
     * * `storage_databases.#.collections.#.name` - Name of the collection.
     * * `storage_databases.#.collections.#.data_sources` -     Array of objects where each object represents a stores data store to map with the collection.
     * * `storage_databases.#.collections.#.data_sources.#.store_name` -     Name of a data store to map to the `<collection>`.
     * * `storage_databases.#.collections.#.data_sources.#.default_format` - Default format that Data Lake assumes if it encounters a file without an extension while searching the storeName.
     * * `storage_databases.#.collections.#.data_sources.#.path` - Controls how Atlas Data Lake searches for and parses files in the storeName before mapping them to the `<collection>`.
     * * `storage_databases.#.views` -     Array of objects where each object represents an [aggregation pipeline](https://docs.mongodb.com/manual/core/aggregation-pipeline/#id1) on a collection.
     * * `storage_databases.#.views.#.name` - Name of the view.
     * * `storage_databases.#.views.#.source` -  Name of the source collection for the view.
     * * `storage_databases.#.views.#.pipeline`- Aggregation pipeline stage(s) to apply to the source collection.
     */
    readonly storageDatabases: outputs.GetDataLakeStorageDatabase[];
    /**
     * Each object in the array represents a data store. Data Lake uses the storage.databases configuration details to map data in each data store to queryable databases and collections.
     * * `storage_stores.#.name` - Name of the data store.
     * * `storage_stores.#.provider` - Defines where the data is stored.
     * * `storage_stores.#.region` - Name of the AWS region in which the S3 bucket is hosted.
     * * `storage_stores.#.bucket` - Name of the AWS S3 bucket.
     * * `storage_stores.#.prefix` - Prefix Data Lake applies when searching for files in the S3 bucket .
     * * `storage_stores.#.delimiter` - The delimiter that separates `storage_databases.#.collections.#.data_sources.#.path` segments in the data store.
     * * `storage_stores.#.include_tags` - Determines whether or not to use S3 tags on the files in the given path as additional partition attributes.
     */
    readonly storageStores: outputs.GetDataLakeStorageStore[];
}

export function getDataLakeOutput(args: GetDataLakeOutputArgs, opts?: pulumi.InvokeOptions): pulumi.Output<GetDataLakeResult> {
    return pulumi.output(args).apply(a => getDataLake(a, opts))
}

/**
 * A collection of arguments for invoking getDataLake.
 */
export interface GetDataLakeOutputArgs {
    /**
     * Name of the data lake.
     */
    name: pulumi.Input<string>;
    /**
     * The unique ID for the project to create a data lake.
     */
    projectId: pulumi.Input<string>;
}

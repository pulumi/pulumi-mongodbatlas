// *** WARNING: this file was generated by pulumi-language-java. ***
// *** Do not edit by hand unless you're certain you know what you are doing! ***

package com.pulumi.mongodbatlas;

import com.pulumi.core.Output;
import com.pulumi.core.annotations.Export;
import com.pulumi.core.annotations.ResourceType;
import com.pulumi.core.internal.Codegen;
import com.pulumi.mongodbatlas.AdvancedClusterArgs;
import com.pulumi.mongodbatlas.Utilities;
import com.pulumi.mongodbatlas.inputs.AdvancedClusterState;
import com.pulumi.mongodbatlas.outputs.AdvancedClusterAdvancedConfiguration;
import com.pulumi.mongodbatlas.outputs.AdvancedClusterBiConnectorConfig;
import com.pulumi.mongodbatlas.outputs.AdvancedClusterConnectionStrings;
import com.pulumi.mongodbatlas.outputs.AdvancedClusterPinnedFcv;
import com.pulumi.mongodbatlas.outputs.AdvancedClusterReplicationSpec;
import com.pulumi.mongodbatlas.outputs.AdvancedClusterTimeouts;
import java.lang.Boolean;
import java.lang.String;
import java.util.List;
import java.util.Map;
import java.util.Optional;
import javax.annotation.Nullable;

/**
 * `mongodbatlas.AdvancedCluster` provides an Advanced Cluster resource. The resource lets you create, edit and delete advanced clusters.
 * 
 * &gt; **IMPORTANT:** If upgrading from our provider versions 1.x.x to 2.0.0 or later, you will be required to update your `mongodbatlas.AdvancedCluster` resource configuration. Please refer this guide for details. This new implementation uses the recommended Terraform Plugin Framework, which, in addition to providing a better user experience and other features, adds support for the `moved` block between different resource types.
 * 
 * &gt; **IMPORTANT:** We recommend all new MongoDB Atlas Terraform users start with the `mongodbatlas.AdvancedCluster` resource.  Key differences between `mongodbatlas.Cluster` and `mongodbatlas.AdvancedCluster` include support for [Multi-Cloud Clusters](https://www.mongodb.com/blog/post/introducing-multicloud-clusters-on-mongodb-atlas), Asymmetric Sharding, and [Independent Scaling of Analytics Node Tiers](https://www.mongodb.com/blog/post/introducing-ability-independently-scale-atlas-analytics-node-tiers). For existing `mongodbatlas.Cluster` resource users see our Migration Guide.
 * 
 * &gt; **IMPORTANT:** When modifying cluster configurations, you may see `(known after apply)` markers for many attributes, even those you haven&#39;t changed. This is expected behavior. See the &#34;known after apply&#34; verbosity section below for details.
 * 
 * &gt; **IMPORTANT:** When configuring auto-scaling, you can now use `useEffectiveFields` to simplify your Terraform workflow. See the Auto-Scaling with Effective Fields section below for details.
 * 
 * &gt; **NOTE:** If Backup Compliance Policy is enabled for the project for which this backup schedule is defined, you cannot modify the backup schedule for an individual cluster below the minimum requirements set in the Backup Compliance Policy.  See [Backup Compliance Policy Prohibited Actions and Considerations](https://www.mongodb.com/docs/atlas/backup/cloud-backup/backup-compliance-policy/#configure-a-backup-compliance-policy).
 * 
 * &gt; **NOTE:** A network container is created for each provider/region combination on the advanced cluster. This can be referenced via a computed attribute for use with other resources. Refer to the `replication_specs[#].container_id` attribute in the Attributes Reference for more information.
 * 
 * &gt; **NOTE:** To enable Cluster Extended Storage Sizes use the `isExtendedStorageSizesEnabled` parameter in the mongodbatlas.Project resource.
 * 
 * &gt; **NOTE:** The Low-CPU instance clusters are prefixed with `R`, for example `R40`. For complete list of Low-CPU instance clusters see Cluster Configuration Options under each [Cloud Provider](https://www.mongodb.com/docs/atlas/reference/cloud-providers).
 * 
 * &gt; **NOTE:** Groups and projects are synonymous terms. You might find groupId in the official documentation.
 * 
 * &gt; **NOTE:** This resource supports Flex clusters. Additionally, you can upgrade M0 clusters to Flex and Flex clusters to Dedicated. When creating a Flex cluster, make sure to set the priority value to 7.
 * 
 * ## Example Usage
 * 
 * ### Example single provider and single region
 * 
 * <pre>
 * {@code
 * package generated_program;
 * 
 * import com.pulumi.Context;
 * import com.pulumi.Pulumi;
 * import com.pulumi.core.Output;
 * import com.pulumi.mongodbatlas.AdvancedCluster;
 * import com.pulumi.mongodbatlas.AdvancedClusterArgs;
 * import com.pulumi.mongodbatlas.inputs.AdvancedClusterReplicationSpecArgs;
 * import java.util.List;
 * import java.util.ArrayList;
 * import java.util.Map;
 * import java.io.File;
 * import java.nio.file.Files;
 * import java.nio.file.Paths;
 * 
 * public class App {
 *     public static void main(String[] args) {
 *         Pulumi.run(App::stack);
 *     }
 * 
 *     public static void stack(Context ctx) {
 *         var this_ = new AdvancedCluster("this", AdvancedClusterArgs.builder()
 *             .projectId("PROJECT ID")
 *             .name("NAME OF CLUSTER")
 *             .clusterType("REPLICASET")
 *             .replicationSpecs(AdvancedClusterReplicationSpecArgs.builder()
 *                 .regionConfigs(AdvancedClusterReplicationSpecRegionConfigArgs.builder()
 *                     .electableSpecs(AdvancedClusterReplicationSpecRegionConfigElectableSpecsArgs.builder()
 *                         .instanceSize("M10")
 *                         .nodeCount(3)
 *                         .build())
 *                     .analyticsSpecs(AdvancedClusterReplicationSpecRegionConfigAnalyticsSpecsArgs.builder()
 *                         .instanceSize("M10")
 *                         .nodeCount(1)
 *                         .build())
 *                     .providerName("AWS")
 *                     .priority(7)
 *                     .regionName("US_EAST_1")
 *                     .build())
 *                 .build())
 *             .build());
 * 
 *     }
 * }
 * }
 * </pre>
 * 
 * ### Example using effective fields with auto-scaling
 * 
 * <pre>
 * {@code
 * package generated_program;
 * 
 * import com.pulumi.Context;
 * import com.pulumi.Pulumi;
 * import com.pulumi.core.Output;
 * import com.pulumi.mongodbatlas.AdvancedCluster;
 * import com.pulumi.mongodbatlas.AdvancedClusterArgs;
 * import com.pulumi.mongodbatlas.inputs.AdvancedClusterReplicationSpecArgs;
 * import com.pulumi.mongodbatlas.MongodbatlasFunctions;
 * import com.pulumi.mongodbatlas.inputs.GetAdvancedClusterArgs;
 * import java.util.List;
 * import java.util.ArrayList;
 * import java.util.Map;
 * import java.io.File;
 * import java.nio.file.Files;
 * import java.nio.file.Paths;
 * 
 * public class App {
 *     public static void main(String[] args) {
 *         Pulumi.run(App::stack);
 *     }
 * 
 *     public static void stack(Context ctx) {
 *         var thisAdvancedCluster = new AdvancedCluster("thisAdvancedCluster", AdvancedClusterArgs.builder()
 *             .projectId(projectId)
 *             .name("auto-scale-cluster")
 *             .clusterType("REPLICASET")
 *             .useEffectiveFields(true)
 *             .replicationSpecs(AdvancedClusterReplicationSpecArgs.builder()
 *                 .regionConfigs(AdvancedClusterReplicationSpecRegionConfigArgs.builder()
 *                     .electableSpecs(AdvancedClusterReplicationSpecRegionConfigElectableSpecsArgs.builder()
 *                         .instanceSize("M10")
 *                         .nodeCount(3)
 *                         .build())
 *                     .autoScaling(AdvancedClusterReplicationSpecRegionConfigAutoScalingArgs.builder()
 *                         .computeEnabled(true)
 *                         .computeScaleDownEnabled(true)
 *                         .computeMinInstanceSize("M10")
 *                         .computeMaxInstanceSize("M30")
 *                         .build())
 *                     .providerName("AWS")
 *                     .priority(7)
 *                     .regionName("US_EAST_1")
 *                     .build())
 *                 .build())
 *             .build());
 * 
 *         // Read the effective (actual) values after Atlas scales
 *         final var this = MongodbatlasFunctions.getAdvancedCluster(GetAdvancedClusterArgs.builder()
 *             .projectId(thisAdvancedCluster.projectId())
 *             .name(thisAdvancedCluster.name())
 *             .useEffectiveFields(true)
 *             .build());
 * 
 *         ctx.export("configuredInstanceSize", this_.applyValue(_this_ -> _this_.replicationSpecs()[0].regionConfigs()[0].electableSpecs().instanceSize()));
 *         ctx.export("actualInstanceSize", this_.applyValue(_this_ -> _this_.replicationSpecs()[0].regionConfigs()[0].effectiveElectableSpecs().instanceSize()));
 *     }
 * }
 * }
 * </pre>
 * 
 * **For module authors:** See the Effective Fields Examples for complete examples of using `useEffectiveFields` and effective specs in reusable Terraform modules.
 * 
 * ### Example Tenant Cluster
 * 
 * <pre>
 * {@code
 * package generated_program;
 * 
 * import com.pulumi.Context;
 * import com.pulumi.Pulumi;
 * import com.pulumi.core.Output;
 * import com.pulumi.mongodbatlas.AdvancedCluster;
 * import com.pulumi.mongodbatlas.AdvancedClusterArgs;
 * import com.pulumi.mongodbatlas.inputs.AdvancedClusterReplicationSpecArgs;
 * import java.util.List;
 * import java.util.ArrayList;
 * import java.util.Map;
 * import java.io.File;
 * import java.nio.file.Files;
 * import java.nio.file.Paths;
 * 
 * public class App {
 *     public static void main(String[] args) {
 *         Pulumi.run(App::stack);
 *     }
 * 
 *     public static void stack(Context ctx) {
 *         var this_ = new AdvancedCluster("this", AdvancedClusterArgs.builder()
 *             .projectId("PROJECT ID")
 *             .name("NAME OF CLUSTER")
 *             .clusterType("REPLICASET")
 *             .replicationSpecs(AdvancedClusterReplicationSpecArgs.builder()
 *                 .regionConfigs(AdvancedClusterReplicationSpecRegionConfigArgs.builder()
 *                     .electableSpecs(AdvancedClusterReplicationSpecRegionConfigElectableSpecsArgs.builder()
 *                         .instanceSize("M0")
 *                         .build())
 *                     .providerName("TENANT")
 *                     .backingProviderName("AWS")
 *                     .regionName("US_EAST_1")
 *                     .priority(7)
 *                     .build())
 *                 .build())
 *             .build());
 * 
 *     }
 * }
 * }
 * </pre>
 * 
 * &gt; **NOTE** Upgrading the tenant cluster to a Flex cluster or a dedicated cluster is supported. When upgrading to a Flex cluster, change the `providerName` from &#34;TENANT&#34; to &#34;FLEX&#34;. See Example Tenant Cluster Upgrade to Flex below. When upgrading to a dedicated cluster, change the `providerName` to your preferred provider (AWS, GCP or Azure) and remove the variable `backingProviderName`. See the Example Tenant Cluster Upgrade below. You can upgrade a tenant cluster only to a single provider on an M10-tier cluster or greater.
 * 
 * When upgrading from the tenant, *only* the upgrade changes will be applied. This helps avoid a corrupt state file in the event that the upgrade succeeds but subsequent updates fail within the same `pulumi up`. To apply additional cluster changes, run a secondary `pulumi up` after the upgrade succeeds.
 * 
 * ### Example Tenant Cluster Upgrade
 * 
 * <pre>
 * {@code
 * package generated_program;
 * 
 * import com.pulumi.Context;
 * import com.pulumi.Pulumi;
 * import com.pulumi.core.Output;
 * import com.pulumi.mongodbatlas.AdvancedCluster;
 * import com.pulumi.mongodbatlas.AdvancedClusterArgs;
 * import com.pulumi.mongodbatlas.inputs.AdvancedClusterReplicationSpecArgs;
 * import java.util.List;
 * import java.util.ArrayList;
 * import java.util.Map;
 * import java.io.File;
 * import java.nio.file.Files;
 * import java.nio.file.Paths;
 * 
 * public class App {
 *     public static void main(String[] args) {
 *         Pulumi.run(App::stack);
 *     }
 * 
 *     public static void stack(Context ctx) {
 *         var this_ = new AdvancedCluster("this", AdvancedClusterArgs.builder()
 *             .projectId("PROJECT ID")
 *             .name("NAME OF CLUSTER")
 *             .clusterType("REPLICASET")
 *             .replicationSpecs(AdvancedClusterReplicationSpecArgs.builder()
 *                 .regionConfigs(AdvancedClusterReplicationSpecRegionConfigArgs.builder()
 *                     .electableSpecs(AdvancedClusterReplicationSpecRegionConfigElectableSpecsArgs.builder()
 *                         .instanceSize("M10")
 *                         .build())
 *                     .providerName("AWS")
 *                     .regionName("US_EAST_1")
 *                     .priority(7)
 *                     .build())
 *                 .build())
 *             .build());
 * 
 *     }
 * }
 * }
 * </pre>
 * 
 * ### Example Tenant Cluster Upgrade to Flex
 * 
 * <pre>
 * {@code
 * package generated_program;
 * 
 * import com.pulumi.Context;
 * import com.pulumi.Pulumi;
 * import com.pulumi.core.Output;
 * import com.pulumi.mongodbatlas.AdvancedCluster;
 * import com.pulumi.mongodbatlas.AdvancedClusterArgs;
 * import com.pulumi.mongodbatlas.inputs.AdvancedClusterReplicationSpecArgs;
 * import java.util.List;
 * import java.util.ArrayList;
 * import java.util.Map;
 * import java.io.File;
 * import java.nio.file.Files;
 * import java.nio.file.Paths;
 * 
 * public class App {
 *     public static void main(String[] args) {
 *         Pulumi.run(App::stack);
 *     }
 * 
 *     public static void stack(Context ctx) {
 *         var this_ = new AdvancedCluster("this", AdvancedClusterArgs.builder()
 *             .projectId("PROJECT ID")
 *             .name("NAME OF CLUSTER")
 *             .clusterType("REPLICASET")
 *             .replicationSpecs(AdvancedClusterReplicationSpecArgs.builder()
 *                 .regionConfigs(AdvancedClusterReplicationSpecRegionConfigArgs.builder()
 *                     .providerName("FLEX")
 *                     .backingProviderName("AWS")
 *                     .regionName("US_EAST_1")
 *                     .priority(7)
 *                     .build())
 *                 .build())
 *             .build());
 * 
 *     }
 * }
 * }
 * </pre>
 * 
 * ### Example Flex Cluster
 * 
 * <pre>
 * {@code
 * package generated_program;
 * 
 * import com.pulumi.Context;
 * import com.pulumi.Pulumi;
 * import com.pulumi.core.Output;
 * import com.pulumi.mongodbatlas.AdvancedCluster;
 * import com.pulumi.mongodbatlas.AdvancedClusterArgs;
 * import com.pulumi.mongodbatlas.inputs.AdvancedClusterReplicationSpecArgs;
 * import java.util.List;
 * import java.util.ArrayList;
 * import java.util.Map;
 * import java.io.File;
 * import java.nio.file.Files;
 * import java.nio.file.Paths;
 * 
 * public class App {
 *     public static void main(String[] args) {
 *         Pulumi.run(App::stack);
 *     }
 * 
 *     public static void stack(Context ctx) {
 *         var this_ = new AdvancedCluster("this", AdvancedClusterArgs.builder()
 *             .projectId("PROJECT ID")
 *             .name("NAME OF CLUSTER")
 *             .clusterType("REPLICASET")
 *             .replicationSpecs(AdvancedClusterReplicationSpecArgs.builder()
 *                 .regionConfigs(AdvancedClusterReplicationSpecRegionConfigArgs.builder()
 *                     .providerName("FLEX")
 *                     .backingProviderName("AWS")
 *                     .regionName("US_EAST_1")
 *                     .priority(7)
 *                     .build())
 *                 .build())
 *             .build());
 * 
 *     }
 * }
 * }
 * </pre>
 * 
 * **NOTE**: Upgrading the Flex cluster is supported. When upgrading from a Flex cluster, change the `providerName` from &#34;TENANT&#34; to your preferred provider (AWS, GCP or Azure) and remove the variable `backingProviderName`.  See the Example Flex Cluster Upgrade below. You can upgrade a Flex cluster only to a single provider on an M10-tier cluster or greater.
 * 
 * When upgrading from a flex cluster, *only* the upgrade changes will be applied. This helps avoid a corrupt state file in the event that the upgrade succeeds but subsequent updates fail within the same `pulumi up`. To apply additional cluster changes, run a secondary `pulumi up` after the upgrade succeeds.
 * 
 * ### Example Flex Cluster Upgrade
 * 
 * <pre>
 * {@code
 * package generated_program;
 * 
 * import com.pulumi.Context;
 * import com.pulumi.Pulumi;
 * import com.pulumi.core.Output;
 * import com.pulumi.mongodbatlas.AdvancedCluster;
 * import com.pulumi.mongodbatlas.AdvancedClusterArgs;
 * import com.pulumi.mongodbatlas.inputs.AdvancedClusterReplicationSpecArgs;
 * import java.util.List;
 * import java.util.ArrayList;
 * import java.util.Map;
 * import java.io.File;
 * import java.nio.file.Files;
 * import java.nio.file.Paths;
 * 
 * public class App {
 *     public static void main(String[] args) {
 *         Pulumi.run(App::stack);
 *     }
 * 
 *     public static void stack(Context ctx) {
 *         var this_ = new AdvancedCluster("this", AdvancedClusterArgs.builder()
 *             .projectId("PROJECT ID")
 *             .name("NAME OF CLUSTER")
 *             .clusterType("REPLICASET")
 *             .replicationSpecs(AdvancedClusterReplicationSpecArgs.builder()
 *                 .regionConfigs(AdvancedClusterReplicationSpecRegionConfigArgs.builder()
 *                     .electableSpecs(AdvancedClusterReplicationSpecRegionConfigElectableSpecsArgs.builder()
 *                         .instanceSize("M10")
 *                         .build())
 *                     .providerName("AWS")
 *                     .regionName("US_EAST_1")
 *                     .priority(7)
 *                     .build())
 *                 .build())
 *             .build());
 * 
 *     }
 * }
 * }
 * </pre>
 * 
 * ### Example Multi-Cloud Cluster
 * <pre>
 * {@code
 * package generated_program;
 * 
 * import com.pulumi.Context;
 * import com.pulumi.Pulumi;
 * import com.pulumi.core.Output;
 * import com.pulumi.mongodbatlas.AdvancedCluster;
 * import com.pulumi.mongodbatlas.AdvancedClusterArgs;
 * import com.pulumi.mongodbatlas.inputs.AdvancedClusterReplicationSpecArgs;
 * import java.util.List;
 * import java.util.ArrayList;
 * import java.util.Map;
 * import java.io.File;
 * import java.nio.file.Files;
 * import java.nio.file.Paths;
 * 
 * public class App {
 *     public static void main(String[] args) {
 *         Pulumi.run(App::stack);
 *     }
 * 
 *     public static void stack(Context ctx) {
 *         var this_ = new AdvancedCluster("this", AdvancedClusterArgs.builder()
 *             .projectId("PROJECT ID")
 *             .name("NAME OF CLUSTER")
 *             .clusterType("REPLICASET")
 *             .replicationSpecs(AdvancedClusterReplicationSpecArgs.builder()
 *                 .regionConfigs(                
 *                     AdvancedClusterReplicationSpecRegionConfigArgs.builder()
 *                         .electableSpecs(AdvancedClusterReplicationSpecRegionConfigElectableSpecsArgs.builder()
 *                             .instanceSize("M10")
 *                             .nodeCount(3)
 *                             .build())
 *                         .analyticsSpecs(AdvancedClusterReplicationSpecRegionConfigAnalyticsSpecsArgs.builder()
 *                             .instanceSize("M10")
 *                             .nodeCount(1)
 *                             .build())
 *                         .providerName("AWS")
 *                         .priority(7)
 *                         .regionName("US_EAST_1")
 *                         .build(),
 *                     AdvancedClusterReplicationSpecRegionConfigArgs.builder()
 *                         .electableSpecs(AdvancedClusterReplicationSpecRegionConfigElectableSpecsArgs.builder()
 *                             .instanceSize("M10")
 *                             .nodeCount(2)
 *                             .build())
 *                         .providerName("GCP")
 *                         .priority(6)
 *                         .regionName("NORTH_AMERICA_NORTHEAST_1")
 *                         .build())
 *                 .build())
 *             .build());
 * 
 *     }
 * }
 * }
 * </pre>
 * ### Example of a Multi Cloud Sharded Cluster with 2 shards
 * 
 * <pre>
 * {@code
 * package generated_program;
 * 
 * import com.pulumi.Context;
 * import com.pulumi.Pulumi;
 * import com.pulumi.core.Output;
 * import com.pulumi.mongodbatlas.AdvancedCluster;
 * import com.pulumi.mongodbatlas.AdvancedClusterArgs;
 * import com.pulumi.mongodbatlas.inputs.AdvancedClusterReplicationSpecArgs;
 * import com.pulumi.mongodbatlas.inputs.AdvancedClusterAdvancedConfigurationArgs;
 * import java.util.List;
 * import java.util.ArrayList;
 * import java.util.Map;
 * import java.io.File;
 * import java.nio.file.Files;
 * import java.nio.file.Paths;
 * 
 * public class App {
 *     public static void main(String[] args) {
 *         Pulumi.run(App::stack);
 *     }
 * 
 *     public static void stack(Context ctx) {
 *         var this_ = new AdvancedCluster("this", AdvancedClusterArgs.builder()
 *             .projectId(project.id())
 *             .name(clusterName)
 *             .clusterType("SHARDED")
 *             .backupEnabled(true)
 *             .replicationSpecs(            
 *                 AdvancedClusterReplicationSpecArgs.builder()
 *                     .regionConfigs(                    
 *                         AdvancedClusterReplicationSpecRegionConfigArgs.builder()
 *                             .electableSpecs(AdvancedClusterReplicationSpecRegionConfigElectableSpecsArgs.builder()
 *                                 .instanceSize("M30")
 *                                 .nodeCount(3)
 *                                 .build())
 *                             .providerName("AWS")
 *                             .priority(7)
 *                             .regionName("US_EAST_1")
 *                             .build(),
 *                         AdvancedClusterReplicationSpecRegionConfigArgs.builder()
 *                             .electableSpecs(AdvancedClusterReplicationSpecRegionConfigElectableSpecsArgs.builder()
 *                                 .instanceSize("M30")
 *                                 .nodeCount(2)
 *                                 .build())
 *                             .providerName("AZURE")
 *                             .priority(6)
 *                             .regionName("US_EAST_2")
 *                             .build())
 *                     .build(),
 *                 AdvancedClusterReplicationSpecArgs.builder()
 *                     .regionConfigs(                    
 *                         AdvancedClusterReplicationSpecRegionConfigArgs.builder()
 *                             .electableSpecs(AdvancedClusterReplicationSpecRegionConfigElectableSpecsArgs.builder()
 *                                 .instanceSize("M30")
 *                                 .nodeCount(3)
 *                                 .build())
 *                             .providerName("AWS")
 *                             .priority(7)
 *                             .regionName("US_EAST_1")
 *                             .build(),
 *                         AdvancedClusterReplicationSpecRegionConfigArgs.builder()
 *                             .electableSpecs(AdvancedClusterReplicationSpecRegionConfigElectableSpecsArgs.builder()
 *                                 .instanceSize("M30")
 *                                 .nodeCount(2)
 *                                 .build())
 *                             .providerName("AZURE")
 *                             .priority(6)
 *                             .regionName("US_EAST_2")
 *                             .build())
 *                     .build())
 *             .advancedConfiguration(AdvancedClusterAdvancedConfigurationArgs.builder()
 *                 .javascriptEnabled(true)
 *                 .oplogSizeMb(991)
 *                 .sampleRefreshIntervalBiConnector(300)
 *                 .build())
 *             .build());
 * 
 *     }
 * }
 * }
 * </pre>
 * 
 * ### Example of a Global Cluster with 2 zones
 * <pre>
 * {@code
 * package generated_program;
 * 
 * import com.pulumi.Context;
 * import com.pulumi.Pulumi;
 * import com.pulumi.core.Output;
 * import com.pulumi.mongodbatlas.AdvancedCluster;
 * import com.pulumi.mongodbatlas.AdvancedClusterArgs;
 * import com.pulumi.mongodbatlas.inputs.AdvancedClusterReplicationSpecArgs;
 * import com.pulumi.mongodbatlas.inputs.AdvancedClusterAdvancedConfigurationArgs;
 * import java.util.List;
 * import java.util.ArrayList;
 * import java.util.Map;
 * import java.io.File;
 * import java.nio.file.Files;
 * import java.nio.file.Paths;
 * 
 * public class App {
 *     public static void main(String[] args) {
 *         Pulumi.run(App::stack);
 *     }
 * 
 *     public static void stack(Context ctx) {
 *         var this_ = new AdvancedCluster("this", AdvancedClusterArgs.builder()
 *             .projectId(project.id())
 *             .name(clusterName)
 *             .clusterType("GEOSHARDED")
 *             .backupEnabled(true)
 *             .replicationSpecs(            
 *                 AdvancedClusterReplicationSpecArgs.builder()
 *                     .zoneName("zone n1")
 *                     .regionConfigs(                    
 *                         AdvancedClusterReplicationSpecRegionConfigArgs.builder()
 *                             .electableSpecs(AdvancedClusterReplicationSpecRegionConfigElectableSpecsArgs.builder()
 *                                 .instanceSize("M30")
 *                                 .nodeCount(3)
 *                                 .build())
 *                             .providerName("AWS")
 *                             .priority(7)
 *                             .regionName("US_EAST_1")
 *                             .build(),
 *                         AdvancedClusterReplicationSpecRegionConfigArgs.builder()
 *                             .electableSpecs(AdvancedClusterReplicationSpecRegionConfigElectableSpecsArgs.builder()
 *                                 .instanceSize("M30")
 *                                 .nodeCount(2)
 *                                 .build())
 *                             .providerName("AZURE")
 *                             .priority(6)
 *                             .regionName("US_EAST_2")
 *                             .build())
 *                     .build(),
 *                 AdvancedClusterReplicationSpecArgs.builder()
 *                     .zoneName("zone n1")
 *                     .regionConfigs(                    
 *                         AdvancedClusterReplicationSpecRegionConfigArgs.builder()
 *                             .electableSpecs(AdvancedClusterReplicationSpecRegionConfigElectableSpecsArgs.builder()
 *                                 .instanceSize("M30")
 *                                 .nodeCount(3)
 *                                 .build())
 *                             .providerName("AWS")
 *                             .priority(7)
 *                             .regionName("US_EAST_1")
 *                             .build(),
 *                         AdvancedClusterReplicationSpecRegionConfigArgs.builder()
 *                             .electableSpecs(AdvancedClusterReplicationSpecRegionConfigElectableSpecsArgs.builder()
 *                                 .instanceSize("M30")
 *                                 .nodeCount(2)
 *                                 .build())
 *                             .providerName("AZURE")
 *                             .priority(6)
 *                             .regionName("US_EAST_2")
 *                             .build())
 *                     .build(),
 *                 AdvancedClusterReplicationSpecArgs.builder()
 *                     .zoneName("zone n2")
 *                     .regionConfigs(                    
 *                         AdvancedClusterReplicationSpecRegionConfigArgs.builder()
 *                             .electableSpecs(AdvancedClusterReplicationSpecRegionConfigElectableSpecsArgs.builder()
 *                                 .instanceSize("M30")
 *                                 .nodeCount(3)
 *                                 .build())
 *                             .providerName("AWS")
 *                             .priority(7)
 *                             .regionName("EU_WEST_1")
 *                             .build(),
 *                         AdvancedClusterReplicationSpecRegionConfigArgs.builder()
 *                             .electableSpecs(AdvancedClusterReplicationSpecRegionConfigElectableSpecsArgs.builder()
 *                                 .instanceSize("M30")
 *                                 .nodeCount(2)
 *                                 .build())
 *                             .providerName("AZURE")
 *                             .priority(6)
 *                             .regionName("EUROPE_NORTH")
 *                             .build())
 *                     .build(),
 *                 AdvancedClusterReplicationSpecArgs.builder()
 *                     .zoneName("zone n2")
 *                     .regionConfigs(                    
 *                         AdvancedClusterReplicationSpecRegionConfigArgs.builder()
 *                             .electableSpecs(AdvancedClusterReplicationSpecRegionConfigElectableSpecsArgs.builder()
 *                                 .instanceSize("M30")
 *                                 .nodeCount(3)
 *                                 .build())
 *                             .providerName("AWS")
 *                             .priority(7)
 *                             .regionName("EU_WEST_1")
 *                             .build(),
 *                         AdvancedClusterReplicationSpecRegionConfigArgs.builder()
 *                             .electableSpecs(AdvancedClusterReplicationSpecRegionConfigElectableSpecsArgs.builder()
 *                                 .instanceSize("M30")
 *                                 .nodeCount(2)
 *                                 .build())
 *                             .providerName("AZURE")
 *                             .priority(6)
 *                             .regionName("EUROPE_NORTH")
 *                             .build())
 *                     .build())
 *             .advancedConfiguration(AdvancedClusterAdvancedConfigurationArgs.builder()
 *                 .javascriptEnabled(true)
 *                 .oplogSizeMb(999)
 *                 .sampleRefreshIntervalBiConnector(300)
 *                 .build())
 *             .build());
 * 
 *     }
 * }
 * }
 * </pre>
 * 
 * ### Example - Return a Connection String
 * Standard
 * <pre>
 * {@code
 * package generated_program;
 * 
 * import com.pulumi.Context;
 * import com.pulumi.Pulumi;
 * import com.pulumi.core.Output;
 * import java.util.List;
 * import java.util.ArrayList;
 * import java.util.Map;
 * import java.io.File;
 * import java.nio.file.Files;
 * import java.nio.file.Paths;
 * 
 * public class App {
 *     public static void main(String[] args) {
 *         Pulumi.run(App::stack);
 *     }
 * 
 *     public static void stack(Context ctx) {
 *         ctx.export("standard", cluster.connectionStrings().standard());
 *     }
 * }
 * }
 * </pre>
 * Standard srv
 * <pre>
 * {@code
 * package generated_program;
 * 
 * import com.pulumi.Context;
 * import com.pulumi.Pulumi;
 * import com.pulumi.core.Output;
 * import java.util.List;
 * import java.util.ArrayList;
 * import java.util.Map;
 * import java.io.File;
 * import java.nio.file.Files;
 * import java.nio.file.Paths;
 * 
 * public class App {
 *     public static void main(String[] args) {
 *         Pulumi.run(App::stack);
 *     }
 * 
 *     public static void stack(Context ctx) {
 *         ctx.export("standardSrv", cluster.connectionStrings().standardSrv());
 *     }
 * }
 * }
 * </pre>
 * Private with Network peering and Custom DNS AWS enabled
 * <pre>
 * {@code
 * package generated_program;
 * 
 * import com.pulumi.Context;
 * import com.pulumi.Pulumi;
 * import com.pulumi.core.Output;
 * import java.util.List;
 * import java.util.ArrayList;
 * import java.util.Map;
 * import java.io.File;
 * import java.nio.file.Files;
 * import java.nio.file.Paths;
 * 
 * public class App {
 *     public static void main(String[] args) {
 *         Pulumi.run(App::stack);
 *     }
 * 
 *     public static void stack(Context ctx) {
 *         ctx.export("private", cluster.connectionStrings().private());
 *     }
 * }
 * }
 * </pre>
 * Private srv with Network peering and Custom DNS AWS enabled
 * <pre>
 * {@code
 * package generated_program;
 * 
 * import com.pulumi.Context;
 * import com.pulumi.Pulumi;
 * import com.pulumi.core.Output;
 * import java.util.List;
 * import java.util.ArrayList;
 * import java.util.Map;
 * import java.io.File;
 * import java.nio.file.Files;
 * import java.nio.file.Paths;
 * 
 * public class App {
 *     public static void main(String[] args) {
 *         Pulumi.run(App::stack);
 *     }
 * 
 *     public static void stack(Context ctx) {
 *         ctx.export("privateSrv", cluster.connectionStrings().privateSrv());
 *     }
 * }
 * }
 * </pre>
 * 
 * By endpointServiceId
 * 
 * Refer to the following for full privatelink endpoint connection string examples:
 * * GCP Private Endpoint
 * * Azure Private Endpoint
 * * AWS, Private Endpoint
 * * AWS, Regionalized Private Endpoints
 * 
 * ### Further Examples
 * 
 * **Cluster Types:**
 * - Replicaset
 * - Symmetric Sharded Cluster
 * - Asymmetric Sharded Cluster
 * - Global Cluster
 * - Multi-Cloud
 * 
 * **Auto-scaling:**
 * - Auto-Scaling Per Shard
 * - Effective Fields Examples
 * 
 * **Upgrades &amp; Migrations:**
 * - Tenant Upgrade
 * - Flex Upgrade
 * - Version Upgrade with Pinned FCV
 * - Migrate Cluster to Advanced Cluster
 * 
 * ## Move
 * 
 * `mongodbatlas__cluster` resources can be moved to `mongodbatlas.AdvancedCluster` in Terraform v1.8 and later, e.g.:
 * 
 * More information about moving resources can be found in our Migration Guide and in the Terraform documentation here and here.
 * 
 * ## Auto-Scaling with Effective Fields
 * 
 * The `useEffectiveFields` attribute enhances auto-scaling workflows by eliminating the need for `lifecycle.ignore_changes` blocks and providing visibility into Atlas-managed changes. This feature only applies to dedicated clusters (M10+) and is not supported for flex and tenant clusters.
 * 
 * ### Why use_effective_fields?
 * 
 * When auto-scaling is enabled on a cluster, Atlas automatically adjusts instance sizes and disk capacity based on workload. Without `useEffectiveFields`, `lifecycle.ignore_changes` blocks are required to prevent Terraform from reverting these Atlas-managed changes. This approach has limitations:
 * 
 * - **Configuration drift**: The actual cluster configuration diverges from your Terraform configuration
 * - **Maintenance overhead**: Careful management of `ignoreChanges` blocks is required, including commenting and uncommenting when making intentional changes
 * - **Limited visibility**: Actual scaled values cannot be easily inspected within Terraform state
 * 
 * ### How useEffectiveFields works
 * 
 * The `useEffectiveFields` attribute changes how the provider handles specification attributes:
 * 
 * **When `useEffectiveFields = false` (default - current behavior):**
 * - Spec attributes (`electableSpecs`, `analyticsSpecs`, `readOnlySpecs`) behavior:
 *   - If values are specified in your Terraform configuration (e.g., `instanceSize = &#34;M10&#34;`), those values remain in your configuration
 *   - If values are not specified, Atlas provides default values automatically
 * - With auto-scaling enabled, Atlas scales your cluster but your configured values do not update to match
 * - This creates plan drift: Terraform shows differences between your configured values and what Atlas has actually deployed
 * - `lifecycle.ignore_changes` must be used to prevent Terraform from reverting Atlas auto-scaling changes back to your original configuration
 * 
 * **When `useEffectiveFields = true` (new behavior):**
 * - **Clear separation of concerns**:
 *   - Spec attributes remain exactly as defined in your Terraform configuration
 *   - Atlas-computed values (defaults and auto-scaled values) are available separately in effective specs
 * - No plan drift occurs when Atlas auto-scales your cluster
 * - Use data sources to read `effectiveElectableSpecs`, `effectiveAnalyticsSpecs`, and `effectiveReadOnlySpecs` for actual values
 * 
 * **Key difference:** With `useEffectiveFields = true`, your configuration stays clean and represents your intent, while effective specs show the reality of what Atlas has provisioned.
 * 
 * See the Example using effective fields with auto-scaling in the Example Usage section.
 * 
 * ### Manually Updating Specs with useEffectiveFields
 * 
 * When `useEffectiveFields = true` and auto-scaling is enabled, you can update `instanceSize`, `diskSizeGb`, or `diskIops` in your configuration at any time without validation errors. However, Atlas echoes these values back in state while continuing to use auto-scaled values for actual cluster operations. To have your configured values take effect, temporarily disable auto-scaling:
 * 
 * 1. Set `computeEnabled = false` and `diskGbEnabled = false` in the `autoScaling` block and apply.
 * 2. Update `instanceSize`, `diskSizeGb`, or `diskIops` to your desired values and apply.
 * 3. Re-enable auto-scaling by setting `computeEnabled` and/or `diskGbEnabled` back to `true` and apply.
 * 
 * This workflow allows you to set specific baseline values from which auto-scaling will resume dynamic adjustments based on workload.
 * 
 * ### Terraform Modules
 * 
 * `useEffectiveFields` is particularly valuable for reusable Terraform modules. Without it, separate module implementations are required (one with lifecycle blocks for auto-scaling, one without). With `useEffectiveFields`, a single module handles both scenarios without lifecycle blocks. See the Effective Fields Examples for complete implementations.
 * 
 * ### Migration path and version 3.x
 * 
 * **Current behavior (provider v2.x):**
 * - `useEffectiveFields` defaults to `false` for full backward compatibility
 * - Set to `true` to opt into the effective fields behavior
 * - The attribute will be deprecated later in v2.x releases in preparation for v3.x
 * 
 * **Future behavior (provider v3.x):**
 * - The effective fields behavior will be enabled by default
 * - The `useEffectiveFields` attribute will be removed, as the new behavior becomes standard
 * - This change will reduce plan verbosity by making specification fields Optional-only (removing Computed), eliminating unnecessary `(known after apply)` markers for user-configured values
 * 
 * **Potential enhancements (v3.x or later):**
 * - If customer demand warrants, effective spec fields (`effectiveElectableSpecs`, `effectiveAnalyticsSpecs`, `effectiveReadOnlySpecs`) may be exposed directly in the resource (currently available only via data source)
 * - This would improve observability by providing direct access to actual operational values from the resource without requiring a separate data source
 * - Note: Effective fields would still show `(known after apply)` markers, but user-configured spec fields would not, resulting in clearer plan output overall
 * 
 * **Migration recommendation:** Adopt `useEffectiveFields = true` in v2.x to prepare for the v3.x transition and benefit from improved auto-scaling workflows immediately. The recommendation is to toggle the flag and remove any existing `lifecycle.ignore_changes` blocks in the same apply, without combining other changes.
 * 
 * ## Considerations and Best Practices
 * 
 * ## Import
 * 
 * Clusters can be imported using project ID and cluster name, in the format `PROJECTID-CLUSTERNAME`, e.g.
 * 
 * ```sh
 * $ pulumi import mongodbatlas:index/advancedCluster:AdvancedCluster my_cluster 1112222b3bf99403840e8934-Cluster0
 * ```
 * 
 * See detailed information for arguments and attributes: [MongoDB API Advanced Clusters](https://docs.atlas.mongodb.com/reference/api/cluster-advanced/create-one-cluster-advanced/)
 * 
 * &gt; **IMPORTANT:**
 * &lt;br&gt; &amp;#8226; When a cluster is imported, the resulting schema structure will always return the new schema including `replicationSpecs` per independent shards of the cluster.
 * 
 */
@ResourceType(type="mongodbatlas:index/advancedCluster:AdvancedCluster")
public class AdvancedCluster extends com.pulumi.resources.CustomResource {
    /**
     * If reconfiguration is necessary to regain a primary due to a regional outage, submit this field alongside your topology reconfiguration to request a new regional outage resistant topology. Forced reconfigurations during an outage of the majority of electable nodes carry a risk of data loss if replicated writes (even majority committed writes) have not been replicated to the new primary node. MongoDB Atlas docs contain more information. To proceed with an operation which carries that risk, set `acceptDataRisksAndForceReplicaSetReconfig` to the current date. Learn more about Reconfiguring a Replica Set during a regional outage [here](https://dochub.mongodb.org/core/regional-outage-reconfigure-replica-set).
     * 
     */
    @Export(name="acceptDataRisksAndForceReplicaSetReconfig", refs={String.class}, tree="[0]")
    private Output</* @Nullable */ String> acceptDataRisksAndForceReplicaSetReconfig;

    /**
     * @return If reconfiguration is necessary to regain a primary due to a regional outage, submit this field alongside your topology reconfiguration to request a new regional outage resistant topology. Forced reconfigurations during an outage of the majority of electable nodes carry a risk of data loss if replicated writes (even majority committed writes) have not been replicated to the new primary node. MongoDB Atlas docs contain more information. To proceed with an operation which carries that risk, set `acceptDataRisksAndForceReplicaSetReconfig` to the current date. Learn more about Reconfiguring a Replica Set during a regional outage [here](https://dochub.mongodb.org/core/regional-outage-reconfigure-replica-set).
     * 
     */
    public Output<Optional<String>> acceptDataRisksAndForceReplicaSetReconfig() {
        return Codegen.optional(this.acceptDataRisksAndForceReplicaSetReconfig);
    }
    /**
     * Additional settings for an Atlas cluster.
     * 
     */
    @Export(name="advancedConfiguration", refs={AdvancedClusterAdvancedConfiguration.class}, tree="[0]")
    private Output<AdvancedClusterAdvancedConfiguration> advancedConfiguration;

    /**
     * @return Additional settings for an Atlas cluster.
     * 
     */
    public Output<AdvancedClusterAdvancedConfiguration> advancedConfiguration() {
        return this.advancedConfiguration;
    }
    /**
     * Flag that indicates whether the cluster can perform backups.
     * If `true`, the cluster can perform backups. You must set this value to `true` for NVMe clusters.
     * 
     * Backup uses:
     * [Cloud Backups](https://docs.atlas.mongodb.com/backup/cloud-backup/overview/#std-label-backup-cloud-provider) for dedicated clusters.
     * [Flex Cluster Backups](https://www.mongodb.com/docs/atlas/backup/cloud-backup/flex-cluster-backup/) for flex clusters.
     * If &#34;`backupEnabled`&#34;  is `false` (default), the cluster doesn&#39;t use Atlas backups.
     * 
     */
    @Export(name="backupEnabled", refs={Boolean.class}, tree="[0]")
    private Output<Boolean> backupEnabled;

    /**
     * @return Flag that indicates whether the cluster can perform backups.
     * If `true`, the cluster can perform backups. You must set this value to `true` for NVMe clusters.
     * 
     * Backup uses:
     * [Cloud Backups](https://docs.atlas.mongodb.com/backup/cloud-backup/overview/#std-label-backup-cloud-provider) for dedicated clusters.
     * [Flex Cluster Backups](https://www.mongodb.com/docs/atlas/backup/cloud-backup/flex-cluster-backup/) for flex clusters.
     * If &#34;`backupEnabled`&#34;  is `false` (default), the cluster doesn&#39;t use Atlas backups.
     * 
     */
    public Output<Boolean> backupEnabled() {
        return this.backupEnabled;
    }
    /**
     * Configuration settings applied to BI Connector for Atlas on this cluster. The MongoDB Connector for Business Intelligence for Atlas (BI Connector) is only available for M10 and larger clusters. The BI Connector is a powerful tool which provides users SQL-based access to their MongoDB databases. As a result, the BI Connector performs operations which may be CPU and memory intensive. Given the limited hardware resources on M10 and M20 cluster tiers, you may experience performance degradation of the cluster when enabling the BI Connector. If this occurs, upgrade to an M30 or larger cluster or disable the BI Connector. See below.
     * 
     */
    @Export(name="biConnectorConfig", refs={AdvancedClusterBiConnectorConfig.class}, tree="[0]")
    private Output<AdvancedClusterBiConnectorConfig> biConnectorConfig;

    /**
     * @return Configuration settings applied to BI Connector for Atlas on this cluster. The MongoDB Connector for Business Intelligence for Atlas (BI Connector) is only available for M10 and larger clusters. The BI Connector is a powerful tool which provides users SQL-based access to their MongoDB databases. As a result, the BI Connector performs operations which may be CPU and memory intensive. Given the limited hardware resources on M10 and M20 cluster tiers, you may experience performance degradation of the cluster when enabling the BI Connector. If this occurs, upgrade to an M30 or larger cluster or disable the BI Connector. See below.
     * 
     */
    public Output<AdvancedClusterBiConnectorConfig> biConnectorConfig() {
        return this.biConnectorConfig;
    }
    /**
     * The cluster ID.
     * 
     */
    @Export(name="clusterId", refs={String.class}, tree="[0]")
    private Output<String> clusterId;

    /**
     * @return The cluster ID.
     * 
     */
    public Output<String> clusterId() {
        return this.clusterId;
    }
    /**
     * Type of the cluster that you want to create.
     * Accepted values include:
     * - `REPLICASET` Replica set
     * - `SHARDED`	Sharded cluster
     * - `GEOSHARDED` Global Cluster
     * 
     */
    @Export(name="clusterType", refs={String.class}, tree="[0]")
    private Output<String> clusterType;

    /**
     * @return Type of the cluster that you want to create.
     * Accepted values include:
     * - `REPLICASET` Replica set
     * - `SHARDED`	Sharded cluster
     * - `GEOSHARDED` Global Cluster
     * 
     */
    public Output<String> clusterType() {
        return this.clusterType;
    }
    /**
     * Config Server Management Mode for creating or updating a sharded cluster. Valid values are `ATLAS_MANAGED` (default) and `FIXED_TO_DEDICATED`. When configured as `ATLAS_MANAGED`, Atlas may automatically switch the cluster&#39;s config server type for optimal performance and savings. When configured as `FIXED_TO_DEDICATED`, the cluster will always use a dedicated config server. To learn more, see the [Sharded Cluster Config Servers documentation](https://dochub.mongodb.org/docs/manual/core/sharded-cluster-config-servers/).
     * 
     */
    @Export(name="configServerManagementMode", refs={String.class}, tree="[0]")
    private Output<String> configServerManagementMode;

    /**
     * @return Config Server Management Mode for creating or updating a sharded cluster. Valid values are `ATLAS_MANAGED` (default) and `FIXED_TO_DEDICATED`. When configured as `ATLAS_MANAGED`, Atlas may automatically switch the cluster&#39;s config server type for optimal performance and savings. When configured as `FIXED_TO_DEDICATED`, the cluster will always use a dedicated config server. To learn more, see the [Sharded Cluster Config Servers documentation](https://dochub.mongodb.org/docs/manual/core/sharded-cluster-config-servers/).
     * 
     */
    public Output<String> configServerManagementMode() {
        return this.configServerManagementMode;
    }
    /**
     * Describes a sharded cluster&#39;s config server type. Valid values are `DEDICATED` and `EMBEDDED`. To learn more, see the [Sharded Cluster Config Servers documentation](https://dochub.mongodb.org/docs/manual/core/sharded-cluster-config-servers/).
     * 
     */
    @Export(name="configServerType", refs={String.class}, tree="[0]")
    private Output<String> configServerType;

    /**
     * @return Describes a sharded cluster&#39;s config server type. Valid values are `DEDICATED` and `EMBEDDED`. To learn more, see the [Sharded Cluster Config Servers documentation](https://dochub.mongodb.org/docs/manual/core/sharded-cluster-config-servers/).
     * 
     */
    public Output<String> configServerType() {
        return this.configServerType;
    }
    /**
     * Set of connection strings that your applications use to connect to this cluster. More information in [Connection-strings](https://docs.mongodb.com/manual/reference/connection-string/). Use the parameters in this object to connect your applications to this cluster. To learn more about the formats of connection strings, see [Connection String Options](https://docs.atlas.mongodb.com/reference/faq/connection-changes/). NOTE: Atlas returns the contents of this object after the cluster is operational, not while it builds the cluster.
     * 
     */
    @Export(name="connectionStrings", refs={AdvancedClusterConnectionStrings.class}, tree="[0]")
    private Output<AdvancedClusterConnectionStrings> connectionStrings;

    /**
     * @return Set of connection strings that your applications use to connect to this cluster. More information in [Connection-strings](https://docs.mongodb.com/manual/reference/connection-string/). Use the parameters in this object to connect your applications to this cluster. To learn more about the formats of connection strings, see [Connection String Options](https://docs.atlas.mongodb.com/reference/faq/connection-changes/). NOTE: Atlas returns the contents of this object after the cluster is operational, not while it builds the cluster.
     * 
     */
    public Output<AdvancedClusterConnectionStrings> connectionStrings() {
        return this.connectionStrings;
    }
    /**
     * Date and time when MongoDB Cloud created this cluster. This parameter expresses its value in ISO 8601 format in UTC.
     * 
     */
    @Export(name="createDate", refs={String.class}, tree="[0]")
    private Output<String> createDate;

    /**
     * @return Date and time when MongoDB Cloud created this cluster. This parameter expresses its value in ISO 8601 format in UTC.
     * 
     */
    public Output<String> createDate() {
        return this.createDate;
    }
    /**
     * Indicates whether to delete the resource being created if a timeout is reached when waiting for completion. When set to `true` and timeout occurs, it triggers the deletion and returns immediately without waiting for deletion to complete. When set to `false`, the timeout will not trigger resource deletion. If you suspect a transient error when the value is `true`, wait before retrying to allow resource deletion to finish. Default is `true`.
     * 
     */
    @Export(name="deleteOnCreateTimeout", refs={Boolean.class}, tree="[0]")
    private Output<Boolean> deleteOnCreateTimeout;

    /**
     * @return Indicates whether to delete the resource being created if a timeout is reached when waiting for completion. When set to `true` and timeout occurs, it triggers the deletion and returns immediately without waiting for deletion to complete. When set to `false`, the timeout will not trigger resource deletion. If you suspect a transient error when the value is `true`, wait before retrying to allow resource deletion to finish. Default is `true`.
     * 
     */
    public Output<Boolean> deleteOnCreateTimeout() {
        return this.deleteOnCreateTimeout;
    }
    /**
     * Possible values are AWS, GCP, AZURE or NONE.  Only needed if you desire to manage the keys, see [Encryption at Rest using Customer Key Management](https://docs.atlas.mongodb.com/security-kms-encryption/) for complete documentation.  You must configure encryption at rest for the Atlas project before enabling it on any cluster in the project. For Documentation, see [AWS](https://docs.atlas.mongodb.com/security-aws-kms/), [GCP](https://docs.atlas.mongodb.com/security-kms-encryption/) and [Azure](https://docs.atlas.mongodb.com/security-azure-kms/#std-label-security-azure-kms). Requirements are if `replication_specs[#].region_configs[#].&lt;type&gt;Specs.instance_size` is M10 or greater and `backupEnabled` is false or omitted.
     * 
     */
    @Export(name="encryptionAtRestProvider", refs={String.class}, tree="[0]")
    private Output<String> encryptionAtRestProvider;

    /**
     * @return Possible values are AWS, GCP, AZURE or NONE.  Only needed if you desire to manage the keys, see [Encryption at Rest using Customer Key Management](https://docs.atlas.mongodb.com/security-kms-encryption/) for complete documentation.  You must configure encryption at rest for the Atlas project before enabling it on any cluster in the project. For Documentation, see [AWS](https://docs.atlas.mongodb.com/security-aws-kms/), [GCP](https://docs.atlas.mongodb.com/security-kms-encryption/) and [Azure](https://docs.atlas.mongodb.com/security-azure-kms/#std-label-security-azure-kms). Requirements are if `replication_specs[#].region_configs[#].&lt;type&gt;Specs.instance_size` is M10 or greater and `backupEnabled` is false or omitted.
     * 
     */
    public Output<String> encryptionAtRestProvider() {
        return this.encryptionAtRestProvider;
    }
    /**
     * Flag that indicates if cluster uses Atlas-Managed Sharding (false, default) or Self-Managed Sharding (true). It can only be enabled for Global Clusters (`GEOSHARDED`). It cannot be changed once the cluster is created. Use this mode if you&#39;re an advanced user and the default configuration is too restrictive for your workload. If you select this option, you must manually configure the sharding strategy, more information [here](https://www.mongodb.com/docs/atlas/tutorial/create-global-cluster/#select-your-sharding-configuration).
     * 
     */
    @Export(name="globalClusterSelfManagedSharding", refs={Boolean.class}, tree="[0]")
    private Output<Boolean> globalClusterSelfManagedSharding;

    /**
     * @return Flag that indicates if cluster uses Atlas-Managed Sharding (false, default) or Self-Managed Sharding (true). It can only be enabled for Global Clusters (`GEOSHARDED`). It cannot be changed once the cluster is created. Use this mode if you&#39;re an advanced user and the default configuration is too restrictive for your workload. If you select this option, you must manually configure the sharding strategy, more information [here](https://www.mongodb.com/docs/atlas/tutorial/create-global-cluster/#select-your-sharding-configuration).
     * 
     */
    public Output<Boolean> globalClusterSelfManagedSharding() {
        return this.globalClusterSelfManagedSharding;
    }
    /**
     * Set that contains key-value pairs between 1 to 255 characters in length for tagging and categorizing the cluster. See below. **DEPRECATED** Use `tags` instead.
     * 
     */
    @Export(name="labels", refs={Map.class,String.class}, tree="[0,1,1]")
    private Output</* @Nullable */ Map<String,String>> labels;

    /**
     * @return Set that contains key-value pairs between 1 to 255 characters in length for tagging and categorizing the cluster. See below. **DEPRECATED** Use `tags` instead.
     * 
     */
    public Output<Optional<Map<String,String>>> labels() {
        return Codegen.optional(this.labels);
    }
    /**
     * Version of the cluster to deploy. Atlas supports all the MongoDB versions that have **not** reached [End of Live](https://www.mongodb.com/legal/support-policy/lifecycles) for M10+ clusters. If omitted, Atlas deploys the cluster with the default version. For more details, see [documentation](https://www.mongodb.com/docs/atlas/reference/faq/database/#which-versions-of-mongodb-do-service-clusters-use-). Atlas always deploys the cluster with the latest stable release of the specified version.  If you set a value to this parameter and set `versionReleaseSystem` `CONTINUOUS`, the resource returns an error. Either clear this parameter or set `versionReleaseSystem`: `LTS`.
     * 
     */
    @Export(name="mongoDbMajorVersion", refs={String.class}, tree="[0]")
    private Output<String> mongoDbMajorVersion;

    /**
     * @return Version of the cluster to deploy. Atlas supports all the MongoDB versions that have **not** reached [End of Live](https://www.mongodb.com/legal/support-policy/lifecycles) for M10+ clusters. If omitted, Atlas deploys the cluster with the default version. For more details, see [documentation](https://www.mongodb.com/docs/atlas/reference/faq/database/#which-versions-of-mongodb-do-service-clusters-use-). Atlas always deploys the cluster with the latest stable release of the specified version.  If you set a value to this parameter and set `versionReleaseSystem` `CONTINUOUS`, the resource returns an error. Either clear this parameter or set `versionReleaseSystem`: `LTS`.
     * 
     */
    public Output<String> mongoDbMajorVersion() {
        return this.mongoDbMajorVersion;
    }
    /**
     * Version of MongoDB the cluster runs, in `major-version`.`minor-version` format.
     * 
     */
    @Export(name="mongoDbVersion", refs={String.class}, tree="[0]")
    private Output<String> mongoDbVersion;

    /**
     * @return Version of MongoDB the cluster runs, in `major-version`.`minor-version` format.
     * 
     */
    public Output<String> mongoDbVersion() {
        return this.mongoDbVersion;
    }
    /**
     * Name of the cluster as it appears in Atlas. Once the cluster is created, its name cannot be changed. **WARNING** Changing the name will result in destruction of the existing cluster and the creation of a new cluster.
     * 
     */
    @Export(name="name", refs={String.class}, tree="[0]")
    private Output<String> name;

    /**
     * @return Name of the cluster as it appears in Atlas. Once the cluster is created, its name cannot be changed. **WARNING** Changing the name will result in destruction of the existing cluster and the creation of a new cluster.
     * 
     */
    public Output<String> name() {
        return this.name;
    }
    /**
     * Flag that indicates whether the cluster is paused or not. You can pause M10 or larger clusters.  You cannot initiate pausing for a shared/tenant tier cluster. If you try to update a `paused` cluster you will get a `CANNOT_UPDATE_PAUSED_CLUSTER` error. See [Considerations for Paused Clusters](https://docs.atlas.mongodb.com/pause-terminate-cluster/#considerations-for-paused-clusters).
     * **NOTE** Pause lasts for up to 30 days. If you don&#39;t resume the cluster within 30 days, Atlas resumes the cluster.  When the cluster resumption happens Terraform will flag the changed state.  If you wish to keep the cluster paused, reapply your Terraform configuration.   If you prefer to allow the automated change of state to unpaused use:
     * `lifecycle {
     * ignoreChanges = [paused]
     * }`
     * 
     */
    @Export(name="paused", refs={Boolean.class}, tree="[0]")
    private Output<Boolean> paused;

    /**
     * @return Flag that indicates whether the cluster is paused or not. You can pause M10 or larger clusters.  You cannot initiate pausing for a shared/tenant tier cluster. If you try to update a `paused` cluster you will get a `CANNOT_UPDATE_PAUSED_CLUSTER` error. See [Considerations for Paused Clusters](https://docs.atlas.mongodb.com/pause-terminate-cluster/#considerations-for-paused-clusters).
     * **NOTE** Pause lasts for up to 30 days. If you don&#39;t resume the cluster within 30 days, Atlas resumes the cluster.  When the cluster resumption happens Terraform will flag the changed state.  If you wish to keep the cluster paused, reapply your Terraform configuration.   If you prefer to allow the automated change of state to unpaused use:
     * `lifecycle {
     * ignoreChanges = [paused]
     * }`
     * 
     */
    public Output<Boolean> paused() {
        return this.paused;
    }
    /**
     * Pins the Feature Compatibility Version (FCV) to the current MongoDB version with a provided expiration date. To unpin the FCV the `pinnedFcv` attribute must be removed. This operation can take several minutes as the request processes through the MongoDB data plane. Once FCV is unpinned it will not be possible to downgrade the `mongoDbMajorVersion`. It is advised that updates to `pinnedFcv` are done isolated from other cluster changes. If a plan contains multiple changes, the FCV change will be applied first. If FCV is unpinned past the expiration date the `pinnedFcv` attribute must be removed. The following [knowledge hub article](https://kb.corp.mongodb.com/article/000021785/) and [FCV documentation](https://www.mongodb.com/docs/atlas/tutorial/major-version-change/#manage-feature-compatibility--fcv--during-upgrades) can be referenced for more details. See below.
     * 
     */
    @Export(name="pinnedFcv", refs={AdvancedClusterPinnedFcv.class}, tree="[0]")
    private Output</* @Nullable */ AdvancedClusterPinnedFcv> pinnedFcv;

    /**
     * @return Pins the Feature Compatibility Version (FCV) to the current MongoDB version with a provided expiration date. To unpin the FCV the `pinnedFcv` attribute must be removed. This operation can take several minutes as the request processes through the MongoDB data plane. Once FCV is unpinned it will not be possible to downgrade the `mongoDbMajorVersion`. It is advised that updates to `pinnedFcv` are done isolated from other cluster changes. If a plan contains multiple changes, the FCV change will be applied first. If FCV is unpinned past the expiration date the `pinnedFcv` attribute must be removed. The following [knowledge hub article](https://kb.corp.mongodb.com/article/000021785/) and [FCV documentation](https://www.mongodb.com/docs/atlas/tutorial/major-version-change/#manage-feature-compatibility--fcv--during-upgrades) can be referenced for more details. See below.
     * 
     */
    public Output<Optional<AdvancedClusterPinnedFcv>> pinnedFcv() {
        return Codegen.optional(this.pinnedFcv);
    }
    /**
     * Flag that indicates if the cluster uses Continuous Cloud Backup.
     * 
     */
    @Export(name="pitEnabled", refs={Boolean.class}, tree="[0]")
    private Output<Boolean> pitEnabled;

    /**
     * @return Flag that indicates if the cluster uses Continuous Cloud Backup.
     * 
     */
    public Output<Boolean> pitEnabled() {
        return this.pitEnabled;
    }
    /**
     * Unique ID for the project to create the cluster.
     * 
     */
    @Export(name="projectId", refs={String.class}, tree="[0]")
    private Output<String> projectId;

    /**
     * @return Unique ID for the project to create the cluster.
     * 
     */
    public Output<String> projectId() {
        return this.projectId;
    }
    /**
     * Flag that enables or disables log redaction, see the [manual](https://www.mongodb.com/docs/manual/administration/monitoring/#log-redaction) for more information. Use this in conjunction with Encryption at Rest and TLS/SSL (Transport Encryption) to assist compliance with regulatory requirements. **Note**: Changing this setting on a cluster will trigger a rolling restart as soon as the cluster is updated.
     * 
     */
    @Export(name="redactClientLogData", refs={Boolean.class}, tree="[0]")
    private Output<Boolean> redactClientLogData;

    /**
     * @return Flag that enables or disables log redaction, see the [manual](https://www.mongodb.com/docs/manual/administration/monitoring/#log-redaction) for more information. Use this in conjunction with Encryption at Rest and TLS/SSL (Transport Encryption) to assist compliance with regulatory requirements. **Note**: Changing this setting on a cluster will trigger a rolling restart as soon as the cluster is updated.
     * 
     */
    public Output<Boolean> redactClientLogData() {
        return this.redactClientLogData;
    }
    /**
     * Replica set scaling mode for your cluster. Valid values are `WORKLOAD_TYPE`, `SEQUENTIAL` and `NODE_TYPE`. By default, Atlas scales under `WORKLOAD_TYPE`. This mode allows Atlas to scale your analytics nodes in parallel to your operational nodes. When configured as `SEQUENTIAL`, Atlas scales all nodes sequentially. This mode is intended for steady-state workloads and applications performing latency-sensitive secondary reads. When configured as `NODE_TYPE`, Atlas scales your electable nodes in parallel with your read-only and analytics nodes. This mode is intended for large, dynamic workloads requiring frequent and timely cluster tier scaling. This is the fastest scaling strategy, but it might impact latency of workloads when performing extensive secondary reads. [Modify the Replica Set Scaling Mode](https://dochub.mongodb.org/core/scale-nodes)
     * 
     */
    @Export(name="replicaSetScalingStrategy", refs={String.class}, tree="[0]")
    private Output<String> replicaSetScalingStrategy;

    /**
     * @return Replica set scaling mode for your cluster. Valid values are `WORKLOAD_TYPE`, `SEQUENTIAL` and `NODE_TYPE`. By default, Atlas scales under `WORKLOAD_TYPE`. This mode allows Atlas to scale your analytics nodes in parallel to your operational nodes. When configured as `SEQUENTIAL`, Atlas scales all nodes sequentially. This mode is intended for steady-state workloads and applications performing latency-sensitive secondary reads. When configured as `NODE_TYPE`, Atlas scales your electable nodes in parallel with your read-only and analytics nodes. This mode is intended for large, dynamic workloads requiring frequent and timely cluster tier scaling. This is the fastest scaling strategy, but it might impact latency of workloads when performing extensive secondary reads. [Modify the Replica Set Scaling Mode](https://dochub.mongodb.org/core/scale-nodes)
     * 
     */
    public Output<String> replicaSetScalingStrategy() {
        return this.replicaSetScalingStrategy;
    }
    /**
     * List of settings that configure your cluster regions. This attribute has one object per shard representing node configurations in each shard. For replica sets there is only one object representing node configurations. The `replicationSpecs` configuration for all shards within the same zone must be the same, with the exception of `instanceSize` and `diskIops` that can scale independently. Note that independent `diskIops` values are only supported for AWS provisioned IOPS, or Azure regions that support Extended IOPS. See below.
     * 
     */
    @Export(name="replicationSpecs", refs={List.class,AdvancedClusterReplicationSpec.class}, tree="[0,1]")
    private Output<List<AdvancedClusterReplicationSpec>> replicationSpecs;

    /**
     * @return List of settings that configure your cluster regions. This attribute has one object per shard representing node configurations in each shard. For replica sets there is only one object representing node configurations. The `replicationSpecs` configuration for all shards within the same zone must be the same, with the exception of `instanceSize` and `diskIops` that can scale independently. Note that independent `diskIops` values are only supported for AWS provisioned IOPS, or Azure regions that support Extended IOPS. See below.
     * 
     */
    public Output<List<AdvancedClusterReplicationSpec>> replicationSpecs() {
        return this.replicationSpecs;
    }
    /**
     * Set to true to retain backup snapshots for the deleted cluster. This parameter applies to the Delete operation and only affects M10 and above clusters. If you encounter the `CANNOT_DELETE_SNAPSHOT_WITH_BACKUP_COMPLIANCE_POLICY` error code, see how to delete a cluster with Backup Compliance Policy.
     * 
     * &gt; **NOTE** Prior version of provider had parameter as `biConnector` state will migrate it to new value you only need to update parameter in your terraform file
     * 
     */
    @Export(name="retainBackupsEnabled", refs={Boolean.class}, tree="[0]")
    private Output</* @Nullable */ Boolean> retainBackupsEnabled;

    /**
     * @return Set to true to retain backup snapshots for the deleted cluster. This parameter applies to the Delete operation and only affects M10 and above clusters. If you encounter the `CANNOT_DELETE_SNAPSHOT_WITH_BACKUP_COMPLIANCE_POLICY` error code, see how to delete a cluster with Backup Compliance Policy.
     * 
     * &gt; **NOTE** Prior version of provider had parameter as `biConnector` state will migrate it to new value you only need to update parameter in your terraform file
     * 
     */
    public Output<Optional<Boolean>> retainBackupsEnabled() {
        return Codegen.optional(this.retainBackupsEnabled);
    }
    /**
     * Certificate Authority that MongoDB Atlas clusters use. You can specify ISRGROOTX1 (for ISRG Root X1).
     * 
     */
    @Export(name="rootCertType", refs={String.class}, tree="[0]")
    private Output<String> rootCertType;

    /**
     * @return Certificate Authority that MongoDB Atlas clusters use. You can specify ISRGROOTX1 (for ISRG Root X1).
     * 
     */
    public Output<String> rootCertType() {
        return this.rootCertType;
    }
    /**
     * Current state of the cluster. The possible states are:
     * - IDLE
     * - CREATING
     * - UPDATING
     * - DELETING
     * - DELETED
     * - REPAIRING
     * * `replication_specs[#].container_id` - A key-value map of the Network Peering Container ID(s) for the configuration specified in `regionConfigs`. The Container ID is the id of the container created when the first cluster in the region (AWS/Azure) or project (GCP) was created.  The syntax is `&#34;providerName:regionName&#34; = &#34;containerId&#34;`. Example `AWS:US_EAST_1&#34; = &#34;61e0797dde08fb498ca11a71`.
     * 
     */
    @Export(name="stateName", refs={String.class}, tree="[0]")
    private Output<String> stateName;

    /**
     * @return Current state of the cluster. The possible states are:
     * - IDLE
     * - CREATING
     * - UPDATING
     * - DELETING
     * - DELETED
     * - REPAIRING
     * * `replication_specs[#].container_id` - A key-value map of the Network Peering Container ID(s) for the configuration specified in `regionConfigs`. The Container ID is the id of the container created when the first cluster in the region (AWS/Azure) or project (GCP) was created.  The syntax is `&#34;providerName:regionName&#34; = &#34;containerId&#34;`. Example `AWS:US_EAST_1&#34; = &#34;61e0797dde08fb498ca11a71`.
     * 
     */
    public Output<String> stateName() {
        return this.stateName;
    }
    /**
     * Set that contains key-value pairs between 1 to 255 characters in length for tagging and categorizing the cluster. See below.
     * 
     */
    @Export(name="tags", refs={Map.class,String.class}, tree="[0,1,1]")
    private Output</* @Nullable */ Map<String,String>> tags;

    /**
     * @return Set that contains key-value pairs between 1 to 255 characters in length for tagging and categorizing the cluster. See below.
     * 
     */
    public Output<Optional<Map<String,String>>> tags() {
        return Codegen.optional(this.tags);
    }
    /**
     * Flag that indicates whether termination protection is enabled on the cluster. If set to true, MongoDB Cloud won&#39;t delete the cluster. If set to false, MongoDB Cloud will delete the cluster.
     * 
     */
    @Export(name="terminationProtectionEnabled", refs={Boolean.class}, tree="[0]")
    private Output<Boolean> terminationProtectionEnabled;

    /**
     * @return Flag that indicates whether termination protection is enabled on the cluster. If set to true, MongoDB Cloud won&#39;t delete the cluster. If set to false, MongoDB Cloud will delete the cluster.
     * 
     */
    public Output<Boolean> terminationProtectionEnabled() {
        return this.terminationProtectionEnabled;
    }
    /**
     * )
     * 
     */
    @Export(name="timeouts", refs={AdvancedClusterTimeouts.class}, tree="[0]")
    private Output</* @Nullable */ AdvancedClusterTimeouts> timeouts;

    /**
     * @return )
     * 
     */
    public Output<Optional<AdvancedClusterTimeouts>> timeouts() {
        return Codegen.optional(this.timeouts);
    }
    /**
     * Controls how hardware specification fields are returned in the response. When set to true, the non-effective specs (`electableSpecs`, `readOnlySpecs`, `analyticsSpecs`) fields return the hardware specifications that the client provided. When set to false (default), the non-effective specs fields show the **current** hardware specifications. Cluster auto-scaling is the primary cause for differences between initial and current hardware specifications. This opt-in feature enhances auto-scaling workflows by eliminating the need for `lifecycle.ignore_changes` blocks and preventing plan drift from Atlas-managed changes. This attribute applies to dedicated clusters, not to tenant or flex clusters. This attribute will be deprecated in provider version 2.x and removed in 3.x when the new behavior becomes default. See Auto-Scaling with Effective Fields for more details.
     * **Important:** Toggle this flag and remove any existing `lifecycle.ignore_changes` blocks for spec fields in the same apply, without combining other changes. Toggling will result in increased plan verbosity with `(known after apply)` markers, which can be safely ignored. If you previously removed `readOnlySpecs` or `analyticsSpecs` attributes from your configuration, you&#39;ll get a validation error for safety reasons to prevent accidental node loss. To resolve: add the blocks back (to keep nodes) or with `nodeCount = 0` (to delete nodes), apply without toggling the flag, then toggle in a separate apply.
     * 
     */
    @Export(name="useEffectiveFields", refs={Boolean.class}, tree="[0]")
    private Output</* @Nullable */ Boolean> useEffectiveFields;

    /**
     * @return Controls how hardware specification fields are returned in the response. When set to true, the non-effective specs (`electableSpecs`, `readOnlySpecs`, `analyticsSpecs`) fields return the hardware specifications that the client provided. When set to false (default), the non-effective specs fields show the **current** hardware specifications. Cluster auto-scaling is the primary cause for differences between initial and current hardware specifications. This opt-in feature enhances auto-scaling workflows by eliminating the need for `lifecycle.ignore_changes` blocks and preventing plan drift from Atlas-managed changes. This attribute applies to dedicated clusters, not to tenant or flex clusters. This attribute will be deprecated in provider version 2.x and removed in 3.x when the new behavior becomes default. See Auto-Scaling with Effective Fields for more details.
     * **Important:** Toggle this flag and remove any existing `lifecycle.ignore_changes` blocks for spec fields in the same apply, without combining other changes. Toggling will result in increased plan verbosity with `(known after apply)` markers, which can be safely ignored. If you previously removed `readOnlySpecs` or `analyticsSpecs` attributes from your configuration, you&#39;ll get a validation error for safety reasons to prevent accidental node loss. To resolve: add the blocks back (to keep nodes) or with `nodeCount = 0` (to delete nodes), apply without toggling the flag, then toggle in a separate apply.
     * 
     */
    public Output<Optional<Boolean>> useEffectiveFields() {
        return Codegen.optional(this.useEffectiveFields);
    }
    /**
     * Release cadence that Atlas uses for this cluster. This parameter defaults to `LTS`. If you set this field to `CONTINUOUS`, you must omit the `mongoDbMajorVersion` field. Atlas accepts:
     * - `CONTINUOUS`:  Atlas creates your cluster using the most recent MongoDB release. Atlas automatically updates your cluster to the latest major and rapid MongoDB releases as they become available.
     * - `LTS`: Atlas creates your cluster using the latest patch release of the MongoDB version that you specify in the mongoDBMajorVersion field. Atlas automatically updates your cluster to subsequent patch releases of this MongoDB version. Atlas doesn&#39;t update your cluster to newer rapid or major MongoDB releases as they become available.
     * 
     */
    @Export(name="versionReleaseSystem", refs={String.class}, tree="[0]")
    private Output<String> versionReleaseSystem;

    /**
     * @return Release cadence that Atlas uses for this cluster. This parameter defaults to `LTS`. If you set this field to `CONTINUOUS`, you must omit the `mongoDbMajorVersion` field. Atlas accepts:
     * - `CONTINUOUS`:  Atlas creates your cluster using the most recent MongoDB release. Atlas automatically updates your cluster to the latest major and rapid MongoDB releases as they become available.
     * - `LTS`: Atlas creates your cluster using the latest patch release of the MongoDB version that you specify in the mongoDBMajorVersion field. Atlas automatically updates your cluster to subsequent patch releases of this MongoDB version. Atlas doesn&#39;t update your cluster to newer rapid or major MongoDB releases as they become available.
     * 
     */
    public Output<String> versionReleaseSystem() {
        return this.versionReleaseSystem;
    }

    /**
     *
     * @param name The _unique_ name of the resulting resource.
     */
    public AdvancedCluster(java.lang.String name) {
        this(name, AdvancedClusterArgs.Empty);
    }
    /**
     *
     * @param name The _unique_ name of the resulting resource.
     * @param args The arguments to use to populate this resource's properties.
     */
    public AdvancedCluster(java.lang.String name, AdvancedClusterArgs args) {
        this(name, args, null);
    }
    /**
     *
     * @param name The _unique_ name of the resulting resource.
     * @param args The arguments to use to populate this resource's properties.
     * @param options A bag of options that control this resource's behavior.
     */
    public AdvancedCluster(java.lang.String name, AdvancedClusterArgs args, @Nullable com.pulumi.resources.CustomResourceOptions options) {
        super("mongodbatlas:index/advancedCluster:AdvancedCluster", name, makeArgs(args, options), makeResourceOptions(options, Codegen.empty()), false);
    }

    private AdvancedCluster(java.lang.String name, Output<java.lang.String> id, @Nullable AdvancedClusterState state, @Nullable com.pulumi.resources.CustomResourceOptions options) {
        super("mongodbatlas:index/advancedCluster:AdvancedCluster", name, state, makeResourceOptions(options, id), false);
    }

    private static AdvancedClusterArgs makeArgs(AdvancedClusterArgs args, @Nullable com.pulumi.resources.CustomResourceOptions options) {
        if (options != null && options.getUrn().isPresent()) {
            return null;
        }
        return args == null ? AdvancedClusterArgs.Empty : args;
    }

    private static com.pulumi.resources.CustomResourceOptions makeResourceOptions(@Nullable com.pulumi.resources.CustomResourceOptions options, @Nullable Output<java.lang.String> id) {
        var defaultOptions = com.pulumi.resources.CustomResourceOptions.builder()
            .version(Utilities.getVersion())
            .build();
        return com.pulumi.resources.CustomResourceOptions.merge(defaultOptions, options, id);
    }

    /**
     * Get an existing Host resource's state with the given name, ID, and optional extra
     * properties used to qualify the lookup.
     *
     * @param name The _unique_ name of the resulting resource.
     * @param id The _unique_ provider ID of the resource to lookup.
     * @param state
     * @param options Optional settings to control the behavior of the CustomResource.
     */
    public static AdvancedCluster get(java.lang.String name, Output<java.lang.String> id, @Nullable AdvancedClusterState state, @Nullable com.pulumi.resources.CustomResourceOptions options) {
        return new AdvancedCluster(name, id, state, options);
    }
}
